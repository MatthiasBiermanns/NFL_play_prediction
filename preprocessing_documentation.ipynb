{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Initialization"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "import re\n",
            "import matplotlib.pyplot as plt"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "years = list(range(1999, 2024))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "csv_files = [f\"./Data/play_by_play_{year}.csv\" for year in years]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# load each csv file as a dataframe and collect them in a list\n",
            "dataframes = []\n",
            "for csv_file in csv_files:\n",
            "    df = pd.read_csv(csv_file)\n",
            "    dataframes.append(df)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# combine all dataframes into a single one\n",
            "combined_df = pd.concat(dataframes, axis=0)\n",
            "\n",
            "# reset the index of the combined dataframe\n",
            "combined_df.reset_index(drop=True, inplace=True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Data Validity Check\n",
            "Tok & Joel"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# plausibility check of the data set\n",
            "display(combined_df.shape)\n",
            "print(\"number of duplicates:\", combined_df.duplicated().sum())\n",
            "print(\"number of distinct teams ('posteam'):\", combined_df['posteam'].nunique())\n",
            "print(\"different 'posteam_type' attributes:\", list(combined_df['posteam_type'].unique()))\n",
            "print(\"different 'play_type' attributes:\", list(combined_df[\"play_type\"].unique()))\n",
            "display(combined_df[(combined_df['yardline_100'] <= 0) | (combined_df['yardline_100'] >= 100)].shape)\n",
            "display(combined_df[(combined_df['season'] < 1999) | (combined_df['season'] > 2023)].shape)\n",
            "display(combined_df[(combined_df['game_seconds_remaining'] < 0) | (combined_df['game_seconds_remaining'] > 3600)].shape)\n",
            "display(combined_df[(combined_df['half_seconds_remaining'] < 0) | (combined_df['half_seconds_remaining'] > 1800)].shape)\n",
            "display(combined_df[(combined_df['quarter_seconds_remaining'] < 0) | (combined_df['quarter_seconds_remaining'] > 900)].shape)\n",
            "display(combined_df[(combined_df['down'] < 1) | (combined_df['down'] > 4)].shape)\n",
            "display(combined_df[combined_df['ydstogo'] > 99].shape)\n",
            "display(combined_df[combined_df['yards_gained'] > 99].shape)\n",
            "display(combined_df[(combined_df['play_type'] == 'run') & (combined_df['interception'] == 1)].shape)\n",
            "display(combined_df[combined_df['incomplete_pass'] == 1].shape)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# detect NAs\n",
            "for column in combined_df.columns:\n",
            "    print(f\"{column}: {sum(combined_df[column].isna())}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# remove non-pass and -run plays from dataframes\n",
            "combined_df.drop(\n",
            "    combined_df[~combined_df[\"play_type\"].isin([\"pass\", \"run\"])].index,\n",
            "    axis=0,\n",
            "    inplace=True,\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# detect NAs\n",
            "display(combined_df.shape)\n",
            "\n",
            "for column in combined_df.columns:\n",
            "    print(f\"{column}: {sum(combined_df[column].isna())}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pd.set_option(\"display.max_columns\", None)\n",
            "\n",
            "display(combined_df.describe())\n",
            "\n",
            "mean = combined_df['yards_gained'].mean()\n",
            "median = combined_df['yards_gained'].median()\n",
            "\n",
            "# Histogram after removal of Extremal values\n",
            "plt.hist(combined_df['yards_gained'],range=(-20, 40), bins=60, alpha=0.7,)\n",
            "plt.axvline(mean, color = 'red', linestyle = 'dashed', linewidth=2, label='Mean')\n",
            "plt.axvline(median, color = 'green', linestyle = 'dashed', linewidth=2, label='Median')\n",
            "plt.title('Histogram: yards_gained')\n",
            "plt.show()\n",
            "\n",
            "# Calculate skew using scipy.stats.skew\n",
            "skewness = combined_df['yards_gained'].skew()\n",
            "\n",
            "print(f'Skewness using scipy.stats.skew: {skewness}')\n",
            "print('Mean', mean)\n",
            "print('Median', median)\n",
            "print(combined_df['yards_gained'].kurt())\n",
            "\n",
            "pd.reset_option(\"display.max_columns\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Removal of Exceptional Observations\n",
            "Tok"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# with penalty = 1 higher avg yards_gained -> biased (free plays etc)\n",
            "pd.set_option(\"display.max_columns\", None)\n",
            "\n",
            "display(combined_df[combined_df['penalty'] == 1].describe())\n",
            "display(combined_df[combined_df['penalty'] == 0].describe())\n",
            "\n",
            "pd.reset_option(\"display.max_columns\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# drop plays with penalties\n",
            "combined_df.drop(combined_df[combined_df[\"penalty\"] == 1].index, axis=0, inplace=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# drop two point conversion plays\n",
            "combined_df.drop(\n",
            "    combined_df[~combined_df[\"two_point_conv_result\"].isna()].index, axis=0, inplace=True\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# 75 perc. quant. is at 48 sec / half -> only end of half -> biased\n",
            "\n",
            "display(combined_df[combined_df['lateral_reception'] == 1].describe())\n",
            "\n",
            "lateral_reception_entries = combined_df[combined_df['lateral_reception'] == 1]\n",
            "\n",
            "# grouped by 'half_seconds_remaining' and count number of occurrences\n",
            "grouped_data = lateral_reception_entries.groupby('game_seconds_remaining').size().reset_index(name='count')\n",
            "\n",
            "plt.figure(figsize=(10, 6))\n",
            "plt.hist(lateral_reception_entries['game_seconds_remaining'], bins=50, edgecolor='black')\n",
            "plt.title('Number of plays with lateral receptions plotted against remaining game seconds')\n",
            "plt.xlabel('Seconds remaining in the game')\n",
            "plt.ylabel('Number of plays with lateral receptions')\n",
            "plt.grid(True)\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# drop plays with laterals\n",
            "combined_df.drop(\n",
            "    combined_df[combined_df[\"lateral_reception\"] == 1].index, axis=0, inplace=True\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# don't drop -> lateral rushs also end around or trick plays -> same reasoning as above, but other way around\n",
            "\n",
            "display(combined_df[combined_df['lateral_rush'] == 1].describe())\n",
            "\n",
            "lateral_rush_entries = combined_df[combined_df['lateral_rush'] == 1]\n",
            "\n",
            "# grouped by 'half_seconds_remaining' and count number of occurrences\n",
            "grouped_data = lateral_rush_entries.groupby('half_seconds_remaining').size().reset_index(name='count')\n",
            "\n",
            "plt.figure(figsize=(10, 6))\n",
            "plt.hist(lateral_rush_entries['half_seconds_remaining'], bins=20, edgecolor='black')\n",
            "plt.title('Number of plays with lateral rushes plotted against remaining half seconds')\n",
            "plt.xlabel('Seconds remaining in the half')\n",
            "plt.ylabel('Number of plays with lateral receptions')\n",
            "plt.grid(True)\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# don't drop -> doesn't change anything if replayed or not\n",
            "\n",
            "display(combined_df[combined_df['replay_or_challenge'] == 1].describe())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# plays where the snap is mishandled or dropped etc\n",
            "display(combined_df[combined_df['aborted_play'] == 1].describe())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# drop aborted plays\n",
            "combined_df.drop(\n",
            "    combined_df[combined_df[\"aborted_play\"] == 1].index, axis=0, inplace=True\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# adjust the spread line to the view of the team with possession of the ball\n",
            "combined_df.loc[combined_df['posteam_type'] == 'away', 'spread_line'] *= -1"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Removal of Non-Decisive Features\n",
            "Thilo & Tok"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "drop_columns1 = [\n",
            "    \"play_id\", # -> only for identification\n",
            "    # \"game_id\",\n",
            "    \"old_game_id\", # -> only for identification\n",
            "    \"home_team\", # -> correlated with posteam/defteam\n",
            "    \"away_team\", # -> correlated with posteam/defteam\n",
            "    # \"season_type\",\n",
            "    \"week\", # -> only for identification\n",
            "    \"side_of_field\", # -> correlated with yardline_100\n",
            "    \"game_date\", # -> only for identification\n",
            "    # \"quarter_seconds_remaining\", # -> correlated with game time\n",
            "    \"quarter_end\", # -> not known before the play\n",
            "    \"sp\", # Binary indicator for whether or not a score occurred on the play -> not known before the play\n",
            "    \"time\", # -> exactly the same as quarter seconds remaining, only in other format\n",
            "    \"yrdln\", # -> perfectly correlated with yardline_100\n",
            "    \"ydsnet\", # -> only known at the end of the drive\n",
            "    \"qb_kneel\", # -> all plays already dropped -> always 0\n",
            "    \"qb_spike\", # -> all plays already dropped -> always 0\n",
            "    \"pass_length\", # -> not known before the play\n",
            "    \"pass_location\", # -> not known before the play\n",
            "    \"run_location\", # -> not known before the play\n",
            "    \"run_gap\", # -> not known before the play\n",
            "    \"field_goal_result\", # -> all plays already dropped -> always 0\n",
            "    \"kick_distance\", # -> all plays already dropped -> always 0\n",
            "    \"extra_point_result\", # -> all plays already dropped -> always 0\n",
            "    \"two_point_conv_result\", # -> all plays already dropped -> always 0\n",
            "    \"home_timeouts_remaining\", # -> correlated with posteam/defteam\n",
            "    \"away_timeouts_remaining\", # -> correlated with posteam/defteam\n",
            "    \"timeout\", # -> all plays already dropped -> always 0\n",
            "    \"timeout_team\", # -> all plays already dropped -> always 0\n",
            "    \"td_team\", # -> not known before the play\n",
            "    \"td_player_name\", # -> not known before the play\n",
            "    \"td_player_id\", # -> not known before the play\n",
            "    # \"posteam_timeouts_remaining\",\n",
            "    # \"defteam_timeouts_remaining\",\n",
            "    \"total_home_score\", # Score for the home team at the end of the play -> not known before the play\n",
            "    \"total_away_score\", # Score for the away team at the end of the play -> not known before the play\n",
            "    \"posteam_score\", # -> not known before the play\n",
            "    \"defteam_score\", # -> not known before the play\n",
            "    \"posteam_score_post\", # -> not known before the play\n",
            "    \"defteam_score_post\", # -> not known before the play\n",
            "    \"score_differential_post\", # -> not known before the play\n",
            "    \"no_score_prob\", # -> based on expected points model -> bias for our model\n",
            "    \"opp_fg_prob\", # -> based on expected points model -> bias for our model\n",
            "    \"opp_safety_prob\", # -> based on expected points model -> bias for our model\n",
            "    \"opp_td_prob\", # -> based on expected points model -> bias for our model\n",
            "    \"fg_prob\", # -> based on expected points model -> bias for our model\n",
            "    \"safety_prob\", # -> based on expected points model -> bias for our model\n",
            "    \"extra_point_prob\", # -> based on expected points model -> bias for our model\n",
            "    \"two_point_conversion_prob\", # -> based on expected points model -> bias for our model\n",
            "    \"total_home_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_away_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_home_rush_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_away_rush_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_home_pass_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_away_pass_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"air_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"yac_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"comp_air_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"comp_yac_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_home_comp_air_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_away_comp_air_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_home_comp_yac_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_away_comp_yac_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_home_raw_air_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_away_raw_air_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_home_raw_yac_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"total_away_raw_yac_epa\", # -> based on expected points model -> bias for our model\n",
            "    \"vegas_wpa\",  # -> not known before the play\n",
            "    \"vegas_home_wpa\",  # -> not known before the play\n",
            "    \"home_wp_post\",  # -> not known before the play\n",
            "    \"away_wp_post\",  # -> not known before the play\n",
            "    \"total_home_rush_wpa\", # -> not known before the play\n",
            "    \"total_away_rush_wpa\", # -> not known before the play\n",
            "    \"total_home_pass_wpa\", # -> not known before the play\n",
            "    \"total_away_pass_wpa\", # -> not known before the play\n",
            "    \"air_wpa\", # -> not known before the play\n",
            "    \"yac_wpa\", # -> not known before the play\n",
            "    \"comp_air_wpa\", # -> not known before the play\n",
            "    \"comp_yac_wpa\", # -> not known before the play\n",
            "    \"total_home_comp_air_wpa\", # -> not known before the play\n",
            "    \"total_away_comp_air_wpa\", # -> not known before the play\n",
            "    \"total_home_comp_yac_wpa\", # -> not known before the play\n",
            "    \"total_away_comp_yac_wpa\", # -> not known before the play\n",
            "    \"total_home_raw_air_wpa\", # -> not known before the play\n",
            "    \"total_away_raw_air_wpa\", # -> not known before the play\n",
            "    \"total_home_raw_yac_wpa\", # -> not known before the play\n",
            "    \"total_away_raw_yac_wpa\", # -> not known before the play\n",
            "    \"punt_blocked\", # -> all plays already dropped -> always 0\n",
            "    \"touchback\", # -> all plays already dropped -> always 0\n",
            "]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "drop_columns2 = [\n",
            "    \"punt_attempt\", # -> all plays already dropped -> always 0\n",
            "    \"safety_player_name\", # -> irrelevant for use case\n",
            "    \"punt_inside_twenty\", # -> all plays already dropped -> always 0\n",
            "    \"kicker_player_name\", # -> all plays already dropped -> always 0\n",
            "    \"passing_yards\", # -> not known before the play\n",
            "    \"interception_player_name\", # -> not known before the play\n",
            "    \"lateral_kickoff_returner_player_id\", # -> all plays already dropped -> always 0\n",
            "    \"assist_tackle\", # -> not known before the play\n",
            "    \"qb_hit_2_player_id\", # -> not known before the play\n",
            "    \"penalty_team\", # -> not known before the play\n",
            "    \"lateral_receiver_player_name\", # -> not known before the play\n",
            "    \"lateral_recovery\", # -> not known before the play\n",
            "    \"rush_touchdown\", # -> not known before the play\n",
            "    \"defensive_two_point_conv\", # -> all plays already dropped -> always 0\n",
            "    \"receiver_jersey_number\", # -> not known before the play\n",
            "    \"tackle_with_assist\", # -> not known before the play\n",
            "    \"fumbled_1_player_name\", # -> not known before the play\n",
            "    \"tackle_with_assist_2_player_id\", # -> not known before the play\n",
            "    \"two_point_attempt\", # -> all plays already dropped -> always 0\n",
            "    \"drive_quarter_start\", # Numeric value indicating in which quarter the given drive has started -> correlated with game time\n",
            "    \"lateral_receiving_yards\", # -> not known before the play\n",
            "    \"end_yard_line\", # -> not known before the play\n",
            "    \"defensive_extra_point_attempt\", # -> all plays already dropped -> always 0\n",
            "    \"pass_touchdown\", # -> not known before the play\n",
            "    \"lateral_rusher_player_name\", # -> not known before the play\n",
            "    \"rusher_id\", # -> not known before the play\n",
            "    \"aborted_play\", # -> all plays already dropped -> always 0\n",
            "    \"drive_yards_penalized\", # Numeric value of how many yards the offense gained or lost through penalties in the given drive -> irrelevant for use case\n",
            "    \"fumble_not_forced\", # -> not known before the play\n",
            "    \"penalty_player_id\", # -> not known before the play\n",
            "    \"tackle_for_loss_2_player_id\", # -> irrelevant for use case\n",
            "    \"drive_end_yard_line\", # -> not known before the play\n",
            "    # \"passer_id\",\n",
            "    \"stadium_id\", # -> irrelevant for use case\n",
            "    \"sack_player_name\", # -> not known before the play\n",
            "    \"punt_out_of_bounds\", # -> all plays already dropped -> always 0\n",
            "    \"tackle_with_assist_2_team\", # -> not known before the play\n",
            "    \"kickoff_downed\", # -> all plays already dropped -> always 0\n",
            "    \"extra_point_attempt\", # -> all plays already dropped -> always 0\n",
            "    \"punt_fair_catch\", # -> all plays already dropped -> always 0\n",
            "    \"assist_tackle_2_player_name\", # -> not known before the play\n",
            "    \"fumble_forced\", # -> not known before the play\n",
            "    \"special_teams_play\", # -> all plays already dropped -> always 0\n",
            "    \"drive_ended_with_score\", # -> not known before the play\n",
            "    \"half_sack_1_player_name\", # -> not known before the play\n",
            "    \"stadium\", # -> correlated with hometeam\n",
            "    \"lateral_receiver_player_id\", # -> not known before the play\n",
            "    \"lateral_sack_player_name\", # -> not known before the play\n",
            "    \"play\", # -> not known before the play\n",
            "    \"tackle_with_assist_1_player_id\", # -> not known before the play\n",
            "    \"forced_fumble_player_2_team\", # -> not known before the play\n",
            "    \"home_coach\", # -> irrelevant for use case (why home_coach and not pos_coach)\n",
            "    \"xyac_epa\", # -> not known before the play\n",
            "    \"lateral_punt_returner_player_name\", # -> not known before the play\n",
            "    \"fantasy\", # -> irrelevant for use case\n",
            "    \"solo_tackle_1_team\", # -> not known before the play\n",
            "    \"drive_start_transition\", # -> irrelevant for use case\n",
            "    \"first_down\", # -> not known before the play\n",
            "    \"first_down_rush\", # -> not known before the play\n",
            "    \"first_down_pass\", # -> not known before the play\n",
            "    \"game_stadium\", # -> correlated with hometeam\n",
            "    \"xyac_fd\", # -> not known before the play\n",
            "    \"drive_play_count\",  # -> only known at the end of the drive\n",
            "    \"passer\", # -> correlated with passer id\n",
            "    \"fumbled_1_player_id\", # -> not known before the play\n",
            "    \"replay_or_challenge_result\", # -> not known before the play\n",
            "    \"drive_real_start_time\", # Local day time when the drive started -> irrelevant for use case\n",
            "    \"receiver_player_id\", # -> not known before the play\n",
            "    \"solo_tackle_2_player_id\", # -> not known before the play\n",
            "    \"fumbled_2_player_name\", # -> not known before the play\n",
            "    \"qb_hit_1_player_name\", # -> not known before the play\n",
            "    \"kickoff_attempt\", # -> all plays already dropped -> always 0\n",
            "    \"xyac_success\", # -> not known before the play\n",
            "    # \"season\",\n",
            "    \"rush\", # -> all other plays already dropped -> always 1 in the run dataframe (always 0 in pass df)\n",
            "    \"tackle_with_assist_2_player_name\", # -> not known before the play\n",
            "    \"assist_tackle_2_team\", # -> not known before the play\n",
            "    \"sack_player_id\", # -> not known before the play\n",
            "    \"assist_tackle_1_team\", # -> not known before the play\n",
            "    \"play_deleted\", # -> all values for deleted plays are nan -> will be eliminated later -> always 0\n",
            "    \"rusher_jersey_number\", # -> not known before the play\n",
            "    \"pass_oe\", # -> only available after 2006\n",
            "    \"return_team\", # -> all plays already dropped -> always nan\n",
            "    \"tackle_for_loss_2_player_name\", # -> not known before the play\n",
            "    \"time_of_day\", # -> only available after 2011\n",
            "    \"end_clock_time\", # -> not known before the play\n",
            "    \"tackle_with_assist_1_team\", # -> not known before the play\n",
            "    \"home_score\", # -> not known before the play\n",
            "    \"tackle_with_assist_1_player_name\", # -> not known before the play\n",
            "    \"kickoff_inside_twenty\", # -> all plays already dropped -> always 0\n",
            "    \"own_kickoff_recovery_player_id\", # -> all plays already dropped -> always 0\n",
            "    \"lateral_reception\", # -> not known before the play\n",
            "    \"qb_hit_1_player_id\", # -> not known before the play\n",
            "    \"own_kickoff_recovery_td\", # -> all plays already dropped -> always 0\n",
            "    \"pass_defense_2_player_name\", # -> not known before the play\n",
            "    \"jersey_number\", # -> not known before the play\n",
            "    \"punter_player_name\", # -> irrelevant for use case\n",
            "    \"blocked_player_name\", # -> not known before the play\n",
            "    \"pass_defense_1_player_name\", # -> not known before the play\n",
            "    \"xyac_median_yardage\", # -> influence on model\n",
            "    \"st_play_type\", # -> all plays already dropped -> always 0\n",
            "    \"success\", # -> not known before the play\n",
            "    \"penalty_player_name\", # -> not known before the play\n",
            "    \"punt_returner_player_name\", # -> not known before the play\n",
            "    \"return_touchdown\", # -> not known before the play\n",
            "    \"blocked_player_id\", # -> not known before the play\n",
            "    \"assist_tackle_1_player_id\", # -> not known before the play\n",
            "    \"receiving_yards\", # -> not known before the play\n",
            "    \"half_sack_2_player_name\", # -> not known before the play\n",
            "    \"drive_game_clock_start\", # -> irrelevant for single play\n",
            "    \"rusher\", # -> not known before the play\n",
            "    \"pass_defense_1_player_id\", # -> not known before the play\n",
            "    \"touchdown\", # -> not known before the play\n",
            "    \"assist_tackle_4_player_id\", # -> not known before the play\n",
            "    \"lateral_return\", # -> all plays already dropped -> always 0\n",
            "    \"solo_tackle_2_team\", # -> not known before the play\n",
            "    \"kickoff_in_endzone\", # -> all plays already dropped -> always 0\n",
            "    \"fumble_out_of_bounds\", # -> not known before the play\n",
            "    \"return_yards\", # -> not known before the play\n",
            "    \"punt_downed\", # -> not known before the play\n",
            "    \"nfl_api_id\", # -> only for identification\n",
            "    \"defensive_extra_point_conv\", # -> all plays already dropped -> always 0\n",
            "    \"out_of_bounds\", # -> not known before the play\n",
            "    \"lateral_interception_player_name\", # -> not known before the play\n",
            "    \"lateral_rush\", # -> not known before the play\n",
            "    \"interception_player_id\", # -> not known before the play\n",
            "    \"assist_tackle_3_player_name\", # -> not known before the play\n",
            "    \"pass_defense_2_player_id\", # -> not known before the play\n",
            "    \"receiver_player_name\", # -> not known before the play\n",
            "    \"away_score\", # -> not known before the play\n",
            "    \"forced_fumble_player_2_player_name\", # -> not known before the play\n",
            "    \"qb_hit_2_player_name\", # -> not known before the play\n",
            "    \"order_sequence\", # -> only available after 2011\n",
            "    \"lateral_rusher_player_id\", # -> not known before the play\n",
            "    \"punt_returner_player_id\", # -> not known before the play\n",
            "    \"cpoe\", # -> correlated with cp (1-cp = cpoe if compl. and 0-cp if incompl.)\n",
            "    \"punt_in_endzone\",  # -> not known before the play\n",
            "    \"fantasy_player_name\", # -> not known before the play\n",
            "    \"passer_player_name\", # -> correlated with passer id\n",
            "    \"xyac_mean_yardage\", # -> irrelevant for use case\n",
            "    \"fixed_drive\", # -> correlated with drive\n",
            "    \"forced_fumble_player_1_player_name\", # -> not known before the play\n",
            "    \"lateral_interception_player_id\", # -> not known before the play\n",
            "    \"solo_tackle\", # -> not known before the play\n",
            "    \"kickoff_out_of_bounds\", # -> all plays already dropped -> always 0\n",
            "    \"fumbled_2_player_id\", # -> not known before the play\n",
            "    \"fumbled_1_team\", # -> not known before the play\n",
            "    \"defensive_two_point_attempt\", # -> all plays already dropped -> always 0\n",
            "    # \"spread_line\",\n",
            "    \"drive_game_clock_end\", # -> not known before the play\n",
            "    \"home_opening_kickoff\", # -> all plays already dropped -> always 0\n",
            "    \"fantasy_id\", # -> irrelevant for use case\n",
            "    \"forced_fumble_player_1_player_id\", # -> not known before the play\n",
            "    \"away_coach\", # -> irrelevant for use case (why away_coach and not pos_coach)\n",
            "    \"fumbled_2_team\", # -> not known before the play\n",
            "    \"kickoff_fair_catch\", # -> not known before the play\n",
            "    \"half_sack_1_player_id\", # -> not known before the play\n",
            "    \"receiver\", # -> not known before the play\n",
            "    \"punter_player_id\", # -> not known before the play\n",
            "    \"xpass\", # -> only available after 2006\n",
            "    \"replay_or_challenge\", # -> no influence on play\n",
            "    \"rusher_player_name\", # -> not known before the play\n",
            "    \"pass\", # -> all other plays already dropped -> always 1 in the pass dataframe (always 0 in run df)\n",
            "    \"assist_tackle_1_player_name\", # -> not known before the play\n",
            "    \"fixed_drive_result\", # -> not known before the play\n",
            "    \"kickoff_returner_player_id\", # -> all plays already dropped -> always 0\n",
            "    \"forced_fumble_player_1_team\", # -> not known before the play\n",
            "    \"half_sack_2_player_id\", # -> not known before the play\n",
            "    \"id\", # -> only for identification\n",
            "    \"drive_end_transition\", # -> not known before the play\n",
            "    \"passer_player_id\", # -> correlated with passer id\n",
            "    \"tackle_for_loss_1_player_name\", # -> not known before the play\n",
            "    \"field_goal_attempt\", # -> all plays already dropped -> always 0\n",
            "    \"lateral_punt_returner_player_id\", # -> all plays already dropped -> always 0\n",
            "    \"play_type_nfl\", # -> correlated with play type\n",
            "    \"drive_first_downs\", # -> correlated with drive play count\n",
            "    \"result\", # -> home_score - away_score\n",
            "    \"receiver_id\", # -> not known before the play\n",
            "    \"start_time\", # -> too many unique values (175)\n",
            "    \"name\", # -> -> only for identification\n",
            "    \"rusher_player_id\", # -> not known before the play\n",
            "    \"passer_jersey_number\", # -> correlated with passer id\n",
            "    \"fantasy_player_id\", # -> not known before the play\n",
            "    \"tackle_for_loss_1_player_id\", # -> not known before the play\n",
            "    \"own_kickoff_recovery_player_name\", # -> all plays already dropped -> always 0\n",
            "    \"drive_time_of_possession\", # -> not known before the play\n",
            "    \"forced_fumble_player_2_player_id\", # -> not known before the play\n",
            "    \"assist_tackle_2_player_id\", # -> not known before the play\n",
            "    \"own_kickoff_recovery\", # -> all plays already dropped -> always 0\n",
            "    \"solo_tackle_1_player_name\", # -> not known before the play\n",
            "    \"special\", # -> all plays already dropped -> always 0\n",
            "    \"lateral_sack_player_id\", # -> not known before the play\n",
            "    \"lateral_rushing_yards\", # -> not known before the play\n",
            "    \"assist_tackle_3_team\", # -> not known before the play\n",
            "    \"drive_quarter_end\", # -> not known before the play\n",
            "    \"location\", # -> only home or neutral -> neutral only includes the super bowl\n",
            "    \"total\", # -> only known after the game\n",
            "    \"rushing_yards\", # -> not known before the play\n",
            "    # \"total_line\",\n",
            "    \"solo_tackle_1_player_id\", # -> not known before the play\n",
            "    \"assist_tackle_4_player_name\", # -> not known before the play\n",
            "    \"assist_tackle_4_team\", # -> not known before the play\n",
            "    \"safety_player_id\", # -> not known before the play\n",
            "    # \"drive_start_yard_line\",\n",
            "    \"kicker_player_id\", # -> all plays already dropped -> always nan\n",
            "    \"assist_tackle_3_player_id\", # -> not known before the play\n",
            "    \"lateral_kickoff_returner_player_name\", # -> not known before the play\n",
            "    \"kickoff_returner_player_name\", # -> all plays already dropped -> always nan\n",
            "    \"solo_tackle_2_player_name\", # -> not known before the play\n",
            "]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "drop_columns3 = [\n",
            "    \"drive_inside20\", # -> not known before the play\n",
            "    \"penalty\", # -> not known before the play\n",
            "    \"penalty_yards\", # -> not known before the play\n",
            "    \"penalty_type\", # -> not known before the play\n",
            "    \"drive_play_id_ended\", # -> not known before the play\n",
            "    \"drive_play_id_started\", # -> only for identification\n",
            "    \"first_down_penalty\", # -> not known before the play\n",
            "    \"fourth_down_converted\", # -> not known before the play\n",
            "    \"fourth_down_failed\", # -> not known before the play\n",
            "    \"fumble_lost\", # -> not known before the play\n",
            "    \"fumble_recovery_1_player_id\", # -> not known before the play\n",
            "    \"fumble_recovery_1_player_name\", # -> not known before the play\n",
            "    \"fumble_recovery_1_team\", # -> not known before the play\n",
            "    \"fumble_recovery_1_yards\", # -> not known before the play\n",
            "    \"fumble_recovery_2_player_id\", # -> not known before the play\n",
            "    \"fumble_recovery_2_player_name\", # -> not known before the play\n",
            "    \"fumble_recovery_2_team\", # -> not known before the play\n",
            "    \"fumble_recovery_2_yards\", # -> not known before the play\n",
            "    \"pass_attempt\", # -> all other plays already dropped -> always 1 in the pass dataframe (always 0 in run df)\n",
            "    \"qb_epa\", # -> not known before the play\n",
            "    \"rush_attempt\", # -> all other plays already dropped -> always 0 in the pass dataframe (always 1 in run df)\n",
            "    \"safety\", # -> not known before the play\n",
            "    \"series_result\", # -> not known before the play\n",
            "    \"series_success\", # -> not known before the play\n",
            "    \"third_down_converted\", # -> not known before the play\n",
            "    \"third_down_failed\", # -> not known before the play\n",
            "    \"complete_pass\", # -> not known before the play\n",
            "    \"incomplete_pass\", # -> not known before the play\n",
            "    \"sack\", # -> not known before the play\n",
            "    \"tackled_for_loss\", # -> not known before the play\n",
            "    # \"half_seconds_remaining\", # -> correlated with game time\n",
            "    \"game_half\", # -> correlated with game time\n",
            "    \"air_yards\", # -> not known before the play\n",
            "    \"yards_after_catch\", # -> not known before the play\n",
            "    \"epa\", # -> not known before the play\n",
            "    \"wpa\", # -> not known before the play\n",
            "    \"interception\", # -> not known before the play\n",
            "    \"fumble\", # -> not known before the play\n",
            "    \"qb_hit\", # -> not known before the play\n",
            "    \"qb_scramble\", # -> not known before the play\n",
            "]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# drop columns\n",
            "combined_df.drop(drop_columns1, axis=1, inplace=True)\n",
            "combined_df.drop(drop_columns2, axis=1, inplace=True)\n",
            "combined_df.drop(drop_columns3, axis=1, inplace=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pd.set_option(\"display.max_columns\", None)\n",
            "pd.set_option(\"display.max_rows\", None)\n",
            "display(combined_df.head(10))\n",
            "pd.reset_option(\"display.max_columns\")\n",
            "pd.reset_option(\"display.max_rows\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "combined_df.shape"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Splitting\n",
            "Thilo"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "run_df = combined_df[combined_df[\"play_type\"] == \"run\"]\n",
            "run_df.drop([\"play_type\"], axis=1, inplace=True)\n",
            "pass_df = combined_df[combined_df[\"play_type\"] == \"pass\"]\n",
            "pass_df.drop([\"play_type\"], axis=1, inplace=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pd.set_option(\"display.max_columns\", None)\n",
            "pd.set_option(\"display.max_rows\", None)\n",
            "display(run_df.head())\n",
            "display(pass_df.head())\n",
            "pd.reset_option(\"display.max_columns\")\n",
            "pd.reset_option(\"display.max_rows\")\n",
            "\n",
            "run_df.to_csv(\"Data/run_df.csv\")\n",
            "pass_df.to_csv(\"Data/pass_df.csv\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Passer-based Outlier Removal\n",
            "Tok"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "len(list(combined_df[\"passer_id\"].unique()))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Count the number of passes per passer and game\n",
            "passer_game_counts = pass_df.groupby(['passer_id', 'game_id']).size().reset_index(name='count')\n",
            "\n",
            "# Filter the passers with at least 14 pass attempts in at least one game\n",
            "passers_with_14_passes = set(passer_game_counts[passer_game_counts['count'] >= 14]['passer_id'])\n",
            "\n",
            "# Filter out the pass plays where the passer_id is not in the passers_with_14_passes set\n",
            "filtered_qb_pass_plays_df = pass_df[pass_df['passer_id'].isin(passers_with_14_passes)]\n",
            "filtered_noqb_pass_plays_df = pass_df[~pass_df['passer_id'].isin(passers_with_14_passes)]\n",
            "\n",
            "pd.set_option(\"display.max_columns\", None)\n",
            "\n",
            "display(filtered_qb_pass_plays_df.describe())\n",
            "display(filtered_noqb_pass_plays_df.describe())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Count the number of passes per passer and game\n",
            "passer_game_counts = pass_df.groupby(['passer_id', 'season']).size().reset_index(name='count')\n",
            "\n",
            "# Filter the passers with at least 224 pass attempts in at least one season\n",
            "passers_with_min_passes = set(passer_game_counts[passer_game_counts['count'] >= 224]['passer_id'])\n",
            "\n",
            "# Filter out the pass plays where the passer_id is not in the passers_with_min_passes set\n",
            "filtered_pass_plays_df = pass_df[pass_df['passer_id'].isin(passers_with_min_passes)]\n",
            "filtered_nopass_plays_df = pass_df[~pass_df['passer_id'].isin(passers_with_min_passes)]\n",
            "\n",
            "display(filtered_pass_plays_df.describe())\n",
            "display(filtered_nopass_plays_df.describe())\n",
            "\n",
            "pass_df = filtered_pass_plays_df\n",
            "\n",
            "pd.reset_option(\"display.max_columns\", None)\n",
            "\n",
            "print(len(passers_with_min_passes))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Column Transformation and Handling of Missing Values\n",
            "Thilo & Tok"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# detect NAs\n",
            "for column in pass_df.columns:\n",
            "    print(f\"{column}: {sum(pass_df[column].isna())}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for column in run_df.columns:\n",
            "    print(f\"{column}: {sum(run_df[column].isna())}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# drop attributes with >5% of nan values\n",
            "pass_df.drop([\"cp\", \"weather\", \"play_clock\", \"wind\", \"temp\", \"surface\"], axis=1, inplace=True)\n",
            "run_df.drop([\"cp\", \"weather\", \"play_clock\", \"wind\", \"temp\", \"surface\"], axis=1, inplace=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "display(pass_df[(pass_df['roof'].isna())].groupby('game_id').count())\n",
            "display(run_df[(run_df['roof'].isna())].groupby('game_id').count())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\"\n",
            "https://www.nfl.com/games/jaguars-at-texans-2021-reg-1 (closed)\n",
            "https://www.nfl.com/games/eagles-at-falcons-2021-reg-1 (open)\n",
            "https://www.nfl.com/games/seahawks-at-colts-2021-reg-1 (closed)\n",
            "https://www.nfl.com/games/rams-at-colts-2021-reg-2 (closed)\n",
            "https://www.nfl.com/games/panthers-at-texans-2021-reg-3 (closed)\n",
            "https://www.nfl.com/games/football-team-at-falcons-2021-reg-4 (open)\n",
            "https://www.nfl.com/games/patriots-at-texans-2021-reg-5  (closed)\n",
            "https://www.nfl.com/games/texans-at-colts-2021-reg-6 (open)\n",
            "https://www.nfl.com/games/panthers-at-falcons-2021-reg-8 (open)\n",
            "https://www.nfl.com/games/rams-at-texans-2021-reg-8 (closed)\n",
            "https://www.nfl.com/games/titans-at-colts-2021-reg-8 (open)\n",
            "https://www.nfl.com/games/jets-at-colts-2021-reg-9 (closed)\n",
            "https://www.nfl.com/games/jaguars-at-colts-2021-reg-10 (closed)\n",
            "https://www.nfl.com/games/patriots-at-falcons-2021-reg-11 (closed)\n",
            "https://www.nfl.com/games/jets-at-texans-2021-reg-12 (closed)\n",
            "https://www.nfl.com/games/buccaneers-at-colts-2021-reg-12 (closed)\n",
            "https://www.nfl.com/games/colts-at-texans-2021-reg-13 (closed)\n",
            "https://www.nfl.com/games/buccaneers-at-falcons-2021-reg-13 (closed)\n",
            "https://www.nfl.com/games/seahawks-at-texans-2021-reg-14 (open)\n",
            "https://www.nfl.com/games/patriots-at-colts-2021-reg-15 (closed)\n",
            "https://www.nfl.com/games/lions-at-falcons-2021-reg-16 (open)\n",
            "https://www.nfl.com/games/chargers-at-texans-2021-reg-16 (closed)\n",
            "https://www.nfl.com/games/raiders-at-colts-2021-reg-17 (closed)\n",
            "https://www.nfl.com/games/saints-at-falcons-2021-reg-18 (closed)\n",
            "https://www.nfl.com/games/titans-at-texans-2021-reg-18 (closed)\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "closed_roof = [\n",
            "    \"2021_01_JAX_HOU\",\n",
            "    \"2021_01_SEA_IND\",\n",
            "    \"2021_02_LA_IND\",\n",
            "    \"2021_03_CAR_HOU\",\n",
            "    \"2021_05_NE_HOU\",\n",
            "    \"2021_08_LA_HOU\",\n",
            "    \"2021_09_NYJ_IND\",\n",
            "    \"2021_10_JAX_IND\",\n",
            "    \"2021_11_NE_ATL\",\n",
            "    \"2021_12_NYJ_HOU\",\n",
            "    \"2021_12_TB_IND\",\n",
            "    \"2021_13_IND_HOU\",\n",
            "    \"2021_13_TB_ATL\",\n",
            "    \"2021_15_NE_IND\",\n",
            "    \"2021_16_LAC_HOU\",\n",
            "    \"2021_17_LV_IND\",\n",
            "    \"2021_18_NO_ATL\",\n",
            "    \"2021_18_TEN_HOU\"\n",
            "]\n",
            "\n",
            "open_roof = [\n",
            "    \"2021_01_PHI_ATL\",\n",
            "    \"2021_04_WAS_ATL\",\n",
            "    \"2021_06_HOU_IND\",\n",
            "    \"2021_08_CAR_ATL\",\n",
            "    \"2021_08_TEN_IND\",\n",
            "    \"2021_14_SEA_HOU\",\n",
            "    \"2021_16_DET_ATL\"\n",
            "]\n",
            "\n",
            "def update_roof(row):\n",
            "    if any(game_id in row['game_id'] for game_id in open_roof):\n",
            "        return \"open\"\n",
            "    if any(game_id in row['game_id'] for game_id in closed_roof):\n",
            "        return \"closed\"\n",
            "    return row['roof']\n",
            "\n",
            "# Apply the function to update 'roof'\n",
            "pass_df.loc[combined_df['roof'].isna(), 'roof'] = pass_df[pass_df['roof'].isna()].apply(update_roof, axis=1)\n",
            "run_df.loc[combined_df['roof'].isna(), 'roof'] = run_df[run_df['roof'].isna()].apply(update_roof, axis=1)\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "display(pass_df[(pass_df['roof'].isna())].groupby('game_id').count())\n",
            "display(run_df[(run_df['roof'].isna())].groupby('game_id').count())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# drop passer and game id which is not needed anymore\n",
            "run_df.drop([\"passer_id\", \"game_id\"], axis=1, inplace=True)\n",
            "pass_df.drop([\"passer_id\", \"game_id\"], axis=1, inplace=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "run_df = run_df.dropna()\n",
            "pass_df = pass_df.dropna()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for column in pass_df.columns:\n",
            "    print(f\"{column}: {sum(pass_df[column].isna())}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for column in run_df.columns:\n",
            "    print(f\"{column}: {sum(run_df[column].isna())}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def transform_dsyl(row):\n",
            "    value = row[\"drive_start_yard_line\"]\n",
            "\n",
            "    if pd.notna(value) and isinstance(value, str):  # Check if not NaN and is a string\n",
            "        match = re.match(r\"([A-Z]+)(\\d+)\", value)\n",
            "        if match:\n",
            "            team, number = match.groups()\n",
            "            return int(number) if row[\"posteam\"] == team else 100 - int(number)\n",
            "        elif \" \" in value:\n",
            "            # Handle the case where there is a space but no match\n",
            "            return int(value.split()[1])\n",
            "        else:\n",
            "            # Handle the case where there is no space (e.g., '50')\n",
            "            return int(value)\n",
            "    else:\n",
            "        # Handle the case where the value is NaN or not a string\n",
            "        return value\n",
            "\n",
            "pass_df[\"drive_start_yard_line\"] = pass_df.apply(transform_dsyl, axis=1)\n",
            "run_df[\"drive_start_yard_line\"] = run_df.apply(transform_dsyl, axis=1)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Encoding of Categorical Features\n",
            "Thilo"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "# Plotting\n",
            "plt.figure(figsize=(10, 6))\n",
            "\n",
            "sns.barplot(x='season', y='yards_gained', data=run_df, errorbar=None, estimator='mean')\n",
            "sns.regplot(x=np.array(range(len(run_df['season'].unique()))), y=run_df.groupby('season')['yards_gained'].mean().values, scatter=False, ax=plt.gca(), color='red', ci=None)\n",
            "\n",
            "plt.xticks(rotation=45, ha='right')\n",
            "plt.title('Average Rushing Yards Gained Per Season')\n",
            "plt.xlabel('Season')\n",
            "plt.ylabel('Average Yards Gained')\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "# Plotting\n",
            "plt.figure(figsize=(10, 6))\n",
            "\n",
            "sns.barplot(x='season', y='yards_gained', data=pass_df, errorbar=None, estimator='mean')\n",
            "sns.regplot(x=np.array(range(len(pass_df['season'].unique()))), y=pass_df.groupby('season')['yards_gained'].mean().values, scatter=False, ax=plt.gca(), color='red', ci=None)\n",
            "\n",
            "plt.xticks(rotation=45, ha='right')\n",
            "plt.title('Average Passing Yards Gained Per Season')\n",
            "plt.xlabel('Season')\n",
            "plt.ylabel('Average Yards Gained')\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# encode categorical features\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.preprocessing import OneHotEncoder\n",
            "from sklearn.compose import ColumnTransformer\n",
            "\n",
            "# create ColumnTransformer\n",
            "encoder = ColumnTransformer(\n",
            "    transformers=[\n",
            "        (\n",
            "            \"encoder\",\n",
            "            OneHotEncoder(drop=\"first\"),\n",
            "            [\"posteam\", \"posteam_type\", \"roof\", \"defteam\", \"season_type\"],\n",
            "        )\n",
            "    ],\n",
            "    remainder=\"passthrough\",  # include non-transformed columns\n",
            ")\n",
            "encoded_data = encoder.fit_transform(run_df)\n",
            "feature_names = [\n",
            "    item.replace(\"encoder__\", \"\").replace(\"remainder__\", \"\")\n",
            "    for item in encoder.get_feature_names_out()\n",
            "]\n",
            "\n",
            "# convert preprocessed data to DataFrame\n",
            "encoded_run_df = pd.DataFrame(encoded_data, columns=feature_names)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# create ColumnTransformer\n",
            "encoder = ColumnTransformer(\n",
            "    transformers=[\n",
            "        (\n",
            "            \"encoder\",\n",
            "            OneHotEncoder(drop=\"first\"),\n",
            "            [\"posteam\", \"posteam_type\", \"roof\", \"defteam\", \"season_type\"],\n",
            "        )\n",
            "    ],\n",
            "    remainder=\"passthrough\",  # include non-transformed columns\n",
            ")\n",
            "encoded_data = encoder.fit_transform(pass_df)\n",
            "feature_names = [\n",
            "    item.replace(\"encoder__\", \"\").replace(\"remainder__\", \"\")\n",
            "    for item in encoder.get_feature_names_out()\n",
            "]\n",
            "\n",
            "# convert preprocessed data to DataFrame\n",
            "encoded_pass_df = pd.DataFrame(encoded_data, columns=feature_names)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pd.set_option(\"display.max_columns\", None)\n",
            "pd.set_option(\"display.max_rows\", None)\n",
            "display(encoded_run_df.head())\n",
            "display(encoded_pass_df.head())\n",
            "pd.reset_option(\"display.max_columns\")\n",
            "pd.reset_option(\"display.max_rows\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# remove remaining for modelling useless features\n",
            "encoded_run_df.drop([\"desc\"], axis=1, inplace=True)\n",
            "encoded_pass_df.drop([\"desc\"], axis=1, inplace=True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Correlation Analysis\n",
            "Thilo"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import plotly.graph_objects as go\n",
            "import seaborn as sns\n",
            "\n",
            "correlation_matrix = encoded_run_df.corr()\n",
            "column_list=encoded_run_df.columns\n",
            "fig = go.Figure(\n",
            "    data=go.Heatmap(\n",
            "        z=correlation_matrix,\n",
            "        x=list(column_list),\n",
            "        y=list(column_list),\n",
            "        colorscale=\"rdylbu\",\n",
            "        zmin=-1,\n",
            "        zmax=1\n",
            "\n",
            "    )\n",
            ")\n",
            "\n",
            "fig.update_layout(\n",
            "    showlegend=False, width=800, height=800, autosize=False, title=\"Correlation matrix\"\n",
            ")\n",
            "\n",
            "fig.update_yaxes(showticklabels=False, autorange=\"reversed\")\n",
            "fig.update_xaxes(showticklabels=False)\n",
            "sns.heatmap(correlation_matrix,cmap=sns.color_palette(\"coolwarm\", as_cmap=True))\n",
            "\n",
            "fig.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "correlation_matrix = encoded_pass_df.corr()\n",
            "column_list=encoded_pass_df.columns\n",
            "fig = go.Figure(\n",
            "    data=go.Heatmap(\n",
            "        z=correlation_matrix,\n",
            "        x=list(column_list),\n",
            "        y=list(column_list),\n",
            "        colorscale=\"rdylbu\",\n",
            "        zmin=-1,\n",
            "        zmax=1\n",
            "\n",
            "    )\n",
            ")\n",
            "\n",
            "sns.heatmap(correlation_matrix,cmap=sns.color_palette(\"coolwarm\", as_cmap=True))\n",
            "\n",
            "fig.update_layout(\n",
            "    showlegend=False, width=800, height=800, autosize=False, title=\"Correlation matrix\"\n",
            ")\n",
            "\n",
            "fig.update_yaxes(showticklabels=False, autorange=\"reversed\")\n",
            "fig.update_xaxes(showticklabels=False)\n",
            "\n",
            "fig.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# removing highly correlated features (not both but one)\n",
            "encoded_pass_df.drop(['ep', 'wp', 'series', 'qtr', 'drive', 'def_wp', 'vegas_wp', 'home_wp', 'away_wp', 'vegas_home_wp'], axis= 1, inplace= True)\n",
            "encoded_run_df.drop(['ep', 'wp', 'series', 'qtr', 'drive', 'def_wp', 'vegas_wp', 'home_wp', 'away_wp', 'vegas_home_wp'], axis= 1, inplace= True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Outlier Removal\n",
            "Joel"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Imports\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "\n",
            "# set options to work with\n",
            "pd.set_option(\"display.max_columns\", None)\n",
            "pd.set_option(\"display.max_rows\", None)\n",
            "\n",
            "# show first 5 rows of dataframe\n",
            "display(encoded_pass_df.head())\n",
            "\n",
            "# Convert the datatypes of the attributes in the DataFrame\n",
            "for column in encoded_pass_df.columns:\n",
            "    try:\n",
            "        encoded_pass_df[column] = pd.to_numeric(encoded_pass_df[column])\n",
            "    except ValueError:\n",
            "        encoded_pass_df[column] = encoded_pass_df[column].apply(str)\n",
            "\n",
            "# Seperate boolean from numeric values, as they are irrelevant for the purpose filtering outliers\n",
            "boolean_variables = [\n",
            "    'posteam_ATL', 'posteam_BAL', 'posteam_BUF', 'posteam_CAR', 'posteam_CHI', 'posteam_CIN', 'posteam_CLE', 'posteam_DAL', 'posteam_DEN', \n",
            "    'posteam_DET', 'posteam_GB', 'posteam_HOU', 'posteam_IND', 'posteam_JAX', 'posteam_KC', 'posteam_LA', 'posteam_LAC', 'posteam_LV', 'posteam_MIA', 'posteam_MIN',\n",
            "    'posteam_NE', 'posteam_NO', 'posteam_NYG', 'posteam_NYJ', 'posteam_PHI', 'posteam_PIT', 'posteam_SEA', 'posteam_SF', 'posteam_TB', 'posteam_TEN', 'posteam_WAS', \n",
            "    'roof_dome', 'roof_open', 'roof_outdoors', 'goal_to_go', 'shotgun', 'no_huddle', 'qb_dropback', 'div_game',\n",
            "    'posteam_type_home', 'defteam_ATL', 'defteam_BAL', 'defteam_BUF', 'defteam_CAR', 'defteam_CHI', 'defteam_CIN', 'defteam_CLE', 'defteam_DAL', 'defteam_DEN', \n",
            "    'defteam_DET', 'defteam_GB', 'defteam_HOU', 'defteam_IND', 'defteam_JAX', 'defteam_KC', 'defteam_LA', 'defteam_LAC', 'defteam_LV', 'defteam_MIA', 'defteam_MIN', \n",
            "    'defteam_NE', 'defteam_NO', 'defteam_NYG', 'defteam_NYJ', 'defteam_PHI', 'defteam_PIT', 'defteam_SEA', 'defteam_SF', 'defteam_TB', 'defteam_TEN', 'defteam_WAS'\n",
            "    ]\n",
            "\n",
            "# boolean_variables = encoded_df.select_dtypes(include='bool').columns.tolist()\n",
            "\n",
            "for variable in boolean_variables:\n",
            "    encoded_pass_df[variable] = encoded_pass_df[variable].astype(bool)\n",
            "\n",
            "# 1. Create a Copy to work with\n",
            "numeric_df = encoded_pass_df.select_dtypes(include=['number']).copy()\n",
            "\n",
            "print(numeric_df)\n",
            "\n",
            "# 2. Checking ranges and distributions\n",
            "ranges_df = pd.DataFrame(columns=['min','max','mean', 'median', 'quantile1', 'quantile3', 'iqr', 'lower', 'upper'])\n",
            "\n",
            "for column in numeric_df.columns:\n",
            "    min_values = numeric_df[column].min()\n",
            "    max_values = numeric_df[column].max()\n",
            "    mean = numeric_df[column].mean()\n",
            "    median = numeric_df[column].median()\n",
            "\n",
            "    #set quantile\n",
            "    quantile_value = 0.25\n",
            "    q1 = numeric_df[column].quantile(quantile_value)\n",
            "    q3 = numeric_df[column].quantile(1-quantile_value)\n",
            "    iqr = q3-q1\n",
            "    lower_bound = q1 - 3.0 * iqr\n",
            "    upper_bound = q3 + 3.0 * iqr\n",
            "    \n",
            "\n",
            "    ranges_df.loc[column] = [min_values, max_values, mean, median, q1, q3, iqr, lower_bound, upper_bound]\n",
            "\n",
            "    # Display the distributions of each column + the quantiles\n",
            "    # Histogram\n",
            "    plt.hist(numeric_df[column], bins=20, alpha=0.7)\n",
            "    plt.axvline(q1, color='red', linestyle='dashed', linewidth=2, label='quantile 1')\n",
            "    plt.axvline(q3, color='blue', linestyle='dashed', linewidth=2, label='quantile 3')\n",
            "    plt.axvline(lower_bound, color='black', linestyle='dashed', linewidth=2, label='lower_bound')\n",
            "    plt.axvline(upper_bound, color='black', linestyle='dashed', linewidth=2, label='upper_bound')\n",
            "    plt.legend()\n",
            "    plt.title('Histogram: ' + column)\n",
            "    plt.show()\n",
            "\n",
            "# Summary of ranges\n",
            "display(ranges_df)\n",
            "\n",
            "# Analyzing the Diagrams the following statements can be made\n",
            "# a) Most of the values outliers can be explained and are therefor meaningfull for the dataset\n",
            "#       --> Get rid of attributes that are negligible for outlier identification\n",
            "drop_columns = ['yardline_100', 'game_seconds_remaining', 'down']\n",
            "numeric_df.drop(drop_columns, axis=1, inplace=True)\n",
            "\n",
            "# b) the data is already well defined and does not contain too many outliers\n",
            "#       --> Keep only the rows where the values are within 3.0 times the IQR from Q1 and Q3\n",
            "ranges_updated_df = pd.DataFrame(columns=['min','max','mean', 'median'])\n",
            "discarded_rows = pd.DataFrame(columns=encoded_pass_df.columns)\n",
            "\n",
            "# 3. Remove the Rows that hold outliers \n",
            "for column in numeric_df.columns:\n",
            "    lower_bound = ranges_df['lower'][column]\n",
            "    upper_bound = ranges_df['upper'][column]\n",
            "\n",
            "    outliers = encoded_pass_df.loc[(numeric_df[column] < lower_bound) | (numeric_df[column] > upper_bound)]\n",
            "    discarded_rows = pd.concat([discarded_rows, outliers])\n",
            "    encoded_pass_df = encoded_pass_df.loc[~((encoded_pass_df[column] < lower_bound) | (encoded_pass_df[column] > upper_bound))]\n",
            "    numeric_df = numeric_df.loc[~((numeric_df[column]< lower_bound) | (numeric_df[column] > upper_bound))]\n",
            "\n",
            "    # Histogram after removal of Extremal values\n",
            "    plt.hist(numeric_df[column], bins=20, alpha=0.7)\n",
            "    plt.title('Histogram: ' + column)\n",
            "    plt.show()\n",
            "\n",
            "    min_values = numeric_df[column].min()\n",
            "    max_values = numeric_df[column].max()\n",
            "    mean = numeric_df[column].mean()\n",
            "    median = numeric_df[column].median()\n",
            "\n",
            "    ranges_updated_df.loc[column] = [min_values, max_values, mean, median]\n",
            "\n",
            "# 4. Display results\n",
            "display(ranges_updated_df)\n",
            "display(discarded_rows.head())\n",
            "display(encoded_pass_df.head())\n",
            "\n",
            "# reset options\n",
            "pd.reset_option(\"display.max_columns\")\n",
            "pd.reset_option(\"display.max_rows\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# show first 5 rows of dataframe\n",
            "display(encoded_run_df.head())\n",
            "\n",
            "# Convert the datatypes of the attributes in the DataFrame\n",
            "for column in encoded_run_df.columns:\n",
            "    try:\n",
            "        encoded_run_df[column] = pd.to_numeric(encoded_run_df[column])\n",
            "    except ValueError:\n",
            "        encoded_run_df[column] = encoded_run_df[column].apply(str)\n",
            "\n",
            "# Seperate boolean from numeric values, as they are irrelevant for the purpose filtering outliers\n",
            "boolean_variables = [\n",
            "    'posteam_ATL', 'posteam_BAL', 'posteam_BUF', 'posteam_CAR', 'posteam_CHI', 'posteam_CIN', 'posteam_CLE', 'posteam_DAL', 'posteam_DEN', \n",
            "    'posteam_DET', 'posteam_GB', 'posteam_HOU', 'posteam_IND', 'posteam_JAX', 'posteam_KC', 'posteam_LA', 'posteam_LAC', 'posteam_LV', 'posteam_MIA', 'posteam_MIN',\n",
            "    'posteam_NE', 'posteam_NO', 'posteam_NYG', 'posteam_NYJ', 'posteam_PHI', 'posteam_PIT', 'posteam_SEA', 'posteam_SF', 'posteam_TB', 'posteam_TEN', 'posteam_WAS', \n",
            "    'roof_dome', 'roof_open', 'roof_outdoors', 'goal_to_go', 'shotgun', 'no_huddle', 'qb_dropback', 'div_game',\n",
            "    'posteam_type_home', 'defteam_ATL', 'defteam_BAL', 'defteam_BUF', 'defteam_CAR', 'defteam_CHI', 'defteam_CIN', 'defteam_CLE', 'defteam_DAL', 'defteam_DEN', \n",
            "    'defteam_DET', 'defteam_GB', 'defteam_HOU', 'defteam_IND', 'defteam_JAX', 'defteam_KC', 'defteam_LA', 'defteam_LAC', 'defteam_LV', 'defteam_MIA', 'defteam_MIN', \n",
            "    'defteam_NE', 'defteam_NO', 'defteam_NYG', 'defteam_NYJ', 'defteam_PHI', 'defteam_PIT', 'defteam_SEA', 'defteam_SF', 'defteam_TB', 'defteam_TEN', 'defteam_WAS'\n",
            "    ]\n",
            "\n",
            "# boolean_variables = encoded_df.select_dtypes(include='bool').columns.tolist()\n",
            "\n",
            "for variable in boolean_variables:\n",
            "    encoded_run_df[variable] = encoded_run_df[variable].astype(bool)\n",
            "\n",
            "# 1. Create a Copy to work with\n",
            "numeric_df = encoded_run_df.select_dtypes(include=['number']).copy()\n",
            "\n",
            "print(numeric_df)\n",
            "\n",
            "# 2. Checking ranges and distributions\n",
            "ranges_df = pd.DataFrame(columns=['min','max','mean', 'median', 'quantile1', 'quantile3', 'iqr', 'lower', 'upper'])\n",
            "\n",
            "for column in numeric_df.columns:\n",
            "    min_values = numeric_df[column].min()\n",
            "    max_values = numeric_df[column].max()\n",
            "    mean = numeric_df[column].mean()\n",
            "    median = numeric_df[column].median()\n",
            "\n",
            "    #set quantile\n",
            "    quantile_value = 0.25\n",
            "    q1 = numeric_df[column].quantile(quantile_value)\n",
            "    q3 = numeric_df[column].quantile(1-quantile_value)\n",
            "    iqr = q3-q1\n",
            "    lower_bound = q1 - 3.0 * iqr\n",
            "    upper_bound = q3 + 3.0 * iqr\n",
            "    \n",
            "\n",
            "    ranges_df.loc[column] = [min_values, max_values, mean, median, q1, q3, iqr, lower_bound, upper_bound]\n",
            "\n",
            "    # Display the distributions of each column + the quantiles\n",
            "    # Histogram\n",
            "    plt.hist(numeric_df[column], bins=20, alpha=0.7)\n",
            "    plt.axvline(q1, color='red', linestyle='dashed', linewidth=2, label='quantile 1')\n",
            "    plt.axvline(q3, color='blue', linestyle='dashed', linewidth=2, label='quantile 3')\n",
            "    plt.axvline(lower_bound, color='black', linestyle='dashed', linewidth=2, label='lower_bound')\n",
            "    plt.axvline(upper_bound, color='black', linestyle='dashed', linewidth=2, label='upper_bound')\n",
            "    plt.legend()\n",
            "    plt.title('Histogram: ' + column)\n",
            "    plt.show()\n",
            "\n",
            "# Summary of ranges\n",
            "display(ranges_df)\n",
            "\n",
            "# Analyzing the Diagrams the following statements can be made\n",
            "# a) Most of the values outliers can be explained and are therefor meaningfull for the dataset\n",
            "#       --> Get rid of attributes that are negligible for outlier identification\n",
            "drop_columns = ['yardline_100', 'game_seconds_remaining', 'down']\n",
            "numeric_df.drop(drop_columns, axis=1, inplace=True)\n",
            "\n",
            "# b) the data is already well defined and does not contain too many outliers\n",
            "#       --> Keep only the rows where the values are within 3.0 times the IQR from Q1 and Q3\n",
            "ranges_updated_df = pd.DataFrame(columns=['min','max','mean', 'median'])\n",
            "discarded_rows = pd.DataFrame(columns=encoded_run_df.columns)\n",
            "\n",
            "# 3. Remove the Rows that hold outliers \n",
            "for column in numeric_df.columns:\n",
            "    lower_bound = ranges_df['lower'][column]\n",
            "    upper_bound = ranges_df['upper'][column]\n",
            "\n",
            "    outliers = encoded_run_df.loc[(numeric_df[column] < lower_bound) | (numeric_df[column] > upper_bound)]\n",
            "    discarded_rows = pd.concat([discarded_rows, outliers])\n",
            "    encoded_run_df = encoded_run_df.loc[~((encoded_run_df[column] < lower_bound) | (encoded_run_df[column] > upper_bound))]\n",
            "    numeric_df = numeric_df.loc[~((numeric_df[column]< lower_bound) | (numeric_df[column] > upper_bound))]\n",
            "\n",
            "    # Histogram after removal of Extremal values\n",
            "    plt.hist(numeric_df[column], bins=20, alpha=0.7)\n",
            "    plt.title('Histogram: ' + column)\n",
            "    plt.show()\n",
            "\n",
            "    min_values = numeric_df[column].min()\n",
            "    max_values = numeric_df[column].max()\n",
            "    mean = numeric_df[column].mean()\n",
            "    median = numeric_df[column].median()\n",
            "\n",
            "    ranges_updated_df.loc[column] = [min_values, max_values, mean, median]\n",
            "\n",
            "# 4. Display results\n",
            "display(ranges_updated_df)\n",
            "display(discarded_rows.head())\n",
            "display(encoded_run_df.head())\n",
            "\n",
            "# reset options\n",
            "pd.reset_option(\"display.max_columns\")\n",
            "pd.reset_option(\"display.max_rows\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Normalization\n",
            "Thilo"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# undo transformation to boolean values for binary features\n",
            "for variable in boolean_variables:\n",
            "    encoded_run_df[variable] = encoded_run_df[variable].astype(int)\n",
            "    encoded_pass_df[variable] = encoded_pass_df[variable].astype(int)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "numeric_features = [\n",
            "    \"yardline_100\",\n",
            "    \"game_seconds_remaining\",\n",
            "    \"down\",\n",
            "    \"ydstogo\",\n",
            "    \"score_differential\",\n",
            "    \"td_prob\",\n",
            "    \"spread_line\",\n",
            "    \"total_line\",\n",
            "    \"season\",\n",
            "    \"posteam_timeouts_remaining\",\n",
            "    \"defteam_timeouts_remaining\",\n",
            "    'quarter_seconds_remaining',\n",
            "    'half_seconds_remaining',\n",
            "    'drive_start_yard_line'\n",
            "]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# plot histograms to see the approximate distribution\n",
            "for column in numeric_features:\n",
            "   lower_bound = ranges_df['lower'][column]\n",
            "   upper_bound = ranges_df['upper'][column]\n",
            "   numeric_df = encoded_run_df.loc[~((encoded_run_df[column] < lower_bound) | (encoded_run_df[column] > upper_bound))]\n",
            "\n",
            "   # Histogram after removal of Extremal values\n",
            "   plt.hist(numeric_df[column], bins=20, alpha=0.7)\n",
            "   plt.title('Histogram: ' + column)\n",
            "   plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# plot histograms to see the approximate distribution\n",
            "for column in numeric_features:\n",
            "   lower_bound = ranges_df['lower'][column]\n",
            "   upper_bound = ranges_df['upper'][column]\n",
            "   numeric_df = encoded_pass_df.loc[~((encoded_pass_df[column] < lower_bound) | (encoded_pass_df[column] > upper_bound))]\n",
            "\n",
            "   # Histogram after removal of Extremal values\n",
            "   plt.hist(numeric_df[column], bins=20, alpha=0.7)\n",
            "   plt.title('Histogram: ' + column)\n",
            "   plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "only the score_differential looks similar to a normal distribution. Therefore, it will be normalized using mean normalization while all other numerical features will be normalized using min-max-scaling"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "numeric_features.remove('score_differential')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
            "normalization= ColumnTransformer(\n",
            "    transformers=[\n",
            "        ('standardization', StandardScaler(),['score_differential']),\n",
            "        ('minmax', MinMaxScaler(), numeric_features)\n",
            "    ],\n",
            "    remainder='passthrough'  # include non-transformed columns\n",
            "    )\n",
            "\n",
            "normalized_data= normalization.fit_transform(encoded_run_df)\n",
            "feature_names = [item.replace('standardization__', '').replace('minmax__', '').replace('remainder__', '') for item in normalization.get_feature_names_out()]\n",
            "normalized_run_df = pd.DataFrame(normalized_data, columns=feature_names)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "normalization= ColumnTransformer(\n",
            "    transformers=[\n",
            "        ('standardization', StandardScaler(),['score_differential']),\n",
            "        ('minmax', MinMaxScaler(), numeric_features)\n",
            "    ],\n",
            "    remainder='passthrough'  # include non-transformed columns\n",
            "    )\n",
            "\n",
            "normalized_data= normalization.fit_transform(encoded_pass_df)\n",
            "feature_names = [item.replace('standardization__', '').replace('minmax__', '').replace('remainder__', '') for item in normalization.get_feature_names_out()]\n",
            "normalized_pass_df = pd.DataFrame(normalized_data, columns=feature_names)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pd.set_option(\"display.max_columns\", None)\n",
            "pd.set_option(\"display.max_rows\", None)\n",
            "display(normalized_run_df.head())\n",
            "display(normalized_pass_df.head())\n",
            "pd.reset_option(\"display.max_columns\")\n",
            "pd.reset_option(\"display.max_rows\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.2"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
