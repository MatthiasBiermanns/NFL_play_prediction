{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training for NFL play prediction\n",
    "\n",
    "---\n",
    " \n",
    "In this notebook, we train multiple regression and ANN models to predict the yards gained per football play. As a basis we use the preselected data created by the `preprocessing.py``.\n",
    "\n",
    "### Inline preprocessing steps:\n",
    "\n",
    "---\n",
    "\n",
    "As we use 5-fold cross-validation to protect the model against overfitting, we need a dynamic preprocessing appraoch. Therefore a pipeline provided by the `preprocessing.py`` will be used to process the training data of each fold. Further we use a nested cross validation to ensure the quality of our model using selected hyperparameters.\n",
    "\n",
    "\n",
    "### Contributors\n",
    "\n",
    "All contributors are only assigned to their primary task, the teams still interchanged know-how and worked on one anothers approaches.\n",
    "\n",
    "---\n",
    "\n",
    "##### Preprocessing Team\n",
    "\n",
    "- Tim Oliver Krause (1689074)\n",
    "- Jan Thilo Viktorin (1684159)\n",
    "- Joël Pflomm (1634591)\n",
    "\n",
    "##### Model Team\n",
    "\n",
    "- Franziska Köllschen (1981780)\n",
    "- Steffen Hüls (1979863)\n",
    "- Matthias Biermanns (1980701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import preprocessing\n",
    "\n",
    "# import for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# imports for regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from subprocess import call\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static variables\n",
    "FILE_LIST = [f'./Data/play_by_play_{year}.csv' for year in range(1999, 2024)]\n",
    "PREPROCESSOR = preprocessing.NFLPreprocessing(FILE_LIST)\n",
    "TARGET_NAME = 'yards_gained'\n",
    "RANDOM_STATE = 42\n",
    "LABEL_PASS = 'Pass'\n",
    "LABEL_RUN = 'Run'\n",
    "\n",
    "# global variables\n",
    "model_counter = 0\n",
    "plot_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(df, data_fraction):\n",
    "    # set fraction between 0 and 1 (e.g. 0.05 -> 5% df)\n",
    "    df_sampled = df.sample(frac=data_fraction, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_sampled = df_sampled.reset_index(drop=True)\n",
    "\n",
    "    return df_sampled\n",
    "\n",
    "def split_feature_target(df):\n",
    "    features = df.drop(TARGET_NAME, axis=1)\n",
    "    target = df[TARGET_NAME]\n",
    "\n",
    "    return features, target\n",
    "\n",
    "def plot_predicts(y_test, predictions, label):\n",
    "    # Evaluate the models\n",
    "    pass_mse = mean_squared_error(y_test, predictions)\n",
    "    pass_rmse = mean_squared_error(y_test, predictions, squared = False)\n",
    "    pass_msa = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "    # Visualize predictions for passes\n",
    "    plt.scatter(y_test, predictions)\n",
    "    plt.title(label + ' Model: True vs Predicted Yards')\n",
    "    plt.xlabel('True Yards')\n",
    "    plt.ylabel('Predicted Yards')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Mean Squared Error: {pass_mse}\")\n",
    "    print(f\"Mean Absolute Error: {pass_msa}\")\n",
    "    print(f\"Root Mean Squared Error: {pass_rmse}\")\n",
    "\n",
    "def plot_decision_tree(pipeline, label, target_tree=0):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_rf'\n",
    "    # Export the decision tree as a dot file\n",
    "    export_graphviz(pipeline.named_steps['regressor'].estimators_[target_tree], out_file=f'{fileName}.dot', \n",
    "                    feature_names=PREPROCESSOR.get_prepro_feature_names_from_pipeline(),\n",
    "                    rounded=True, proportion=False, \n",
    "                    precision=2, filled=True)\n",
    "\n",
    "    # Convert the dot file to png using Graphviz (make sure Graphviz is installed)\n",
    "    call(['dot', '-Tpng', f'{fileName}.dot', '-o', f'{fileName}.png', '-Gdpi=600'])\n",
    "\n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "    display(Image(filename=f'{fileName}.png'))\n",
    "\n",
    "def plot_decision_tree_xgb(pipeline, label, target_tree=0):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_xgb'\n",
    "\n",
    "    xgb.plot_tree(pipeline.named_steps['regressor'], num_trees=target_tree)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(15, 10)\n",
    "    fig.canvas.manager.set_window_title('Decision Tree for ' + label + ' Model')\n",
    "    fig.savefig(f'{fileName}.png')\n",
    "    \n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "    Image(filename=f'{fileName}.png')\n",
    "\n",
    "def plot_train_val_loss(training_losses, validation_losses, label):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_ann'\n",
    "    # Plot the training and validation loss\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Neural Network Training and Validation Loss - ' + label)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{fileName}.png')\n",
    "    plt.show()\n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "def plot_feature_importances(pipeline, label, show_top_n=10):\n",
    "    # Get feature importances\n",
    "    feature_importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "    # Get the feature names after preprocessing\n",
    "    columns = PREPROCESSOR.get_prepro_feature_names_from_pipeline()\n",
    "\n",
    "    # Create a DataFrame to display feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': feature_importances})\n",
    "\n",
    "    # Sort the DataFrame by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df['Feature'][:show_top_n], feature_importance_df['Importance'][:show_top_n])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top {show_top_n} Feature Importances - {label}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_coef(pipeline):\n",
    "    coefs = pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "    # Get the feature names after preprocessing\n",
    "    columns = PREPROCESSOR.get_prepro_feature_names_from_pipeline()\n",
    "\n",
    "    coef = pd.DataFrame(coefs, columns=[\"Coefficients\"], index=columns)\n",
    "    coef.plot(kind=\"barh\", figsize=(9, 7))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(pipeline, df, data_fraction: float = None, label = None):\n",
    "    data = df.copy()\n",
    "    if(data_fraction and data_fraction < 1.0):\n",
    "        data = get_sample(data, data_fraction)\n",
    "    \n",
    "    features, target = split_feature_target(data)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target)\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = pipeline.predict(X_test)\n",
    "\n",
    "    plot_predicts(y_test, predictions, label if label else '')\n",
    "    save_model(features, target, predictions, label, title_appendix=f'normal_{data_fraction*100}%_of_data')\n",
    "\n",
    "    return y_test, predictions\n",
    "\n",
    "def test_model_k_fold(df, pipeline, label, k_folds: int=3, data_fraction: float = 1.0):\n",
    "    data = df.copy()\n",
    "    if(data_fraction and data_fraction < 1.0):\n",
    "        data = get_sample(df, data_fraction)\n",
    "    \n",
    "    features, target = split_feature_target(data)\n",
    "\n",
    "    cv_predictions = cross_val_predict(pipeline, features, target, cv=k_folds)\n",
    "    \n",
    "    plot_predicts(target, cv_predictions, label)\n",
    "    save_model(features, target, cv_predictions, label, title_appendix=f'{k_folds}_folds_{data_fraction*100}%_of_data')\n",
    "    return cv_predictions\n",
    "\n",
    "def estimate_hyperparams(df, pipeline, scoring, k_folds=3, parameters={}, data_fraction: float = 1.0):\n",
    "    data = df.copy()\n",
    "    if(data_fraction and data_fraction < 1.0):\n",
    "        data = get_sample(df, data_fraction)\n",
    "\n",
    "    features, target = split_feature_target(data)\n",
    "    \n",
    "    # create the grid search instance\n",
    "    grid_search_estimator = GridSearchCV(pipeline, parameters, scoring=scoring, cv=k_folds, return_train_score=False, n_jobs=1)\n",
    "\n",
    "    # run the grid search\n",
    "    grid_search_estimator.fit(features, target)\n",
    "    \n",
    "    display(grid_search_estimator.best_params_)\n",
    "    display(pd.DataFrame(grid_search_estimator.cv_results_))\n",
    "\n",
    "    return grid_search_estimator\n",
    "\n",
    "def generate_param_combinations(parameters):\n",
    "    '''list of list required for cross product of two lists'''\n",
    "    return list(ParameterGrid(parameters))\n",
    "\n",
    "def save_model(features, target, predictions, label, title_appendix: str = ''):\n",
    "    global model_counter\n",
    "\n",
    "    predictions_df = pd.DataFrame({'predicted yards_gained': predictions})\n",
    "    save_model = pd.concat([features, target, predictions_df], axis=1)\n",
    "\n",
    "    if (title_appendix != ''):\n",
    "        title_appendix = '_'+title_appendix\n",
    "    save_model.to_excel(f'./results/model_{model_counter}_{label}{title_appendix}.xlsx')\n",
    "\n",
    "    # increase counter for files\n",
    "    model_counter = model_counter + 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Regressor Class - with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithHistory(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mlp_params=None):\n",
    "        self.mlp_params = mlp_params\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.mlp_regressor = MLPRegressor(**(self.mlp_params or {}))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, X_val, y, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        for epoch in range(self.mlp_regressor.max_iter):\n",
    "            self.mlp_regressor.partial_fit(X, y)\n",
    "\n",
    "            # Calculate training loss\n",
    "            y_train_pred = self.mlp_regressor.predict(X)\n",
    "            training_loss = mean_squared_error(y, y_train_pred)\n",
    "            self.training_losses.append(training_loss)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            y_val_pred = self.mlp_regressor.predict(X_val)\n",
    "            validation_loss = mean_squared_error(y_val, y_val_pred)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.mlp_regressor.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"mlp_params\": self.mlp_params}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.mlp_params = params[\"mlp_params\"]\n",
    "        self.mlp_regressor.set_params(**self.mlp_params)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return -mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_df = PREPROCESSOR.run_df\n",
    "pass_df = PREPROCESSOR.pass_df\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df)\n",
    "pass_features, pass_target = split_feature_target(pass_df)\n",
    "\n",
    "run_X_train, run_X_test, run_y_train, run_y_test = train_test_split(run_features, run_target)\n",
    "pass_X_train, pass_X_test, pass_y_train, pass_y_test = train_test_split(pass_features, pass_target)\n",
    "\n",
    "print(run_features.shape)\n",
    "print(pass_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'strict_factor_iqr': 1.0,\n",
    "        'loose_factor_iqr': 2.0,\n",
    "        'strict_columns': ['yardline_100', 'ydstogo'],\n",
    "        'omit_columns': []\n",
    "    }\n",
    "}\n",
    "\n",
    "pass_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'strict_factor_iqr': 1.5,\n",
    "        'loose_factor_iqr': 3.0,\n",
    "        'strict_columns': ['ydstogo'],\n",
    "        'omit_columns': []\n",
    "    }\n",
    "}\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_y_test, run_predictions = test_model(pipeline, run_df, 0.05, label=LABEL_RUN)\n",
    "plot_coef(pipeline)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_y_test, pass_predictions = test_model(pipeline, pass_df, 0.05, label=LABEL_PASS)\n",
    "plot_coef(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for linear regression\n",
    "\n",
    "# could be more over engineered with automatic dict creation if necessary\n",
    "run_params = {\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'strict_factor_iqr': 1.0,\n",
    "            'loose_factor_iqr': 2.0,\n",
    "            'strict_columns': ['yardline_100', 'ydstogo'],\n",
    "            'omit_columns': []\n",
    "        }, {\n",
    "            'strict_factor_iqr': 1.5,\n",
    "            'loose_factor_iqr': 3.0,\n",
    "            'strict_columns': ['yardline_100', 'ydstogo'],\n",
    "            'omit_columns': []\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "pass_params = {\n",
    "    'outlier_remover__kw_args': [{\n",
    "        'strict_factor_iqr': 1.5,\n",
    "        'loose_factor_iqr': 3.0,\n",
    "        'strict_columns': ['ydstogo'],\n",
    "        'omit_columns': []\n",
    "    }]\n",
    "}\n",
    "\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, pipeline, 'neg_root_mean_squared_error', parameters=run_params, data_fraction=0.25)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pipeline, 'neg_root_mean_squared_error', parameters=pass_params, data_fraction=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic model\n",
    "\n",
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(make_pipeline(PolynomialFeatures(2), LinearRegression()))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(make_pipeline(PolynomialFeatures(2), LinearRegression()))\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.05, label=LABEL_RUN)\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.05, label=LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1. Semester/Data Mining/Projekt/NFL_play_prediction/model.ipynb Cell 20\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pipeline \u001b[39m=\u001b[39m PREPROCESSOR\u001b[39m.\u001b[39mmake_preprocessing_pipeline(poly_pipeline)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m parameters \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mregressor__polynomialfeatures__degree\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m }\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m run_grid_search \u001b[39m=\u001b[39m estimate_hyperparams(run_df, pipeline, \u001b[39m'\u001b[39m\u001b[39mneg_root_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m, parameters\u001b[39m=\u001b[39mparameters, data_fraction\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m pass_grid_search \u001b[39m=\u001b[39m estimate_hyperparams(pass_df, pipeline, \u001b[39m'\u001b[39m\u001b[39mneg_root_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m, parameters\u001b[39m=\u001b[39mparameters, data_fraction\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "\u001b[1;32m/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1. Semester/Data Mining/Projekt/NFL_play_prediction/model.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m grid_search_estimator \u001b[39m=\u001b[39m GridSearchCV(pipeline, parameters, scoring\u001b[39m=\u001b[39mscoring, cv\u001b[39m=\u001b[39mk_folds, return_train_score\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, n_jobs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# run the grid search\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m grid_search_estimator\u001b[39m.\u001b[39mfit(features, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m display(grid_search_estimator\u001b[39m.\u001b[39mbest_params_)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/I538844/Library/CloudStorage/OneDrive-SAPSE/Master/1.%20Semester/Data%20Mining/Projekt/NFL_play_prediction/model.ipynb#Y161sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m display(pd\u001b[39m.\u001b[39mDataFrame(grid_search_estimator\u001b[39m.\u001b[39mcv_results_))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1378\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39m(candidate_params), \u001b[39menumerate\u001b[39m(cv\u001b[39m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mapply_async(batch, callback\u001b[39m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/pipeline.py:297\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    296\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 297\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, yt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:382\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    381\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 382\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_base.py:736\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([out[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs])\n\u001b[1;32m    735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, _, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrank_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingular_ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mlstsq(X, y)\n\u001b[1;32m    737\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT\n\u001b[1;32m    739\u001b[0m \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/linalg/_basic.py:1213\u001b[0m, in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mif\u001b[39;00m real_data:\n\u001b[1;32m   1212\u001b[0m     lwork, iwork \u001b[39m=\u001b[39m _compute_lwork(lapack_lwork, m, n, nrhs, cond)\n\u001b[0;32m-> 1213\u001b[0m     x, s, rank, info \u001b[39m=\u001b[39m lapack_func(a1, b1, lwork,\n\u001b[1;32m   1214\u001b[0m                                    iwork, cond, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1215\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# complex data\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m     lwork, rwork, iwork \u001b[39m=\u001b[39m _compute_lwork(lapack_lwork, m, n,\n\u001b[1;32m   1217\u001b[0m                                          nrhs, cond)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# make new pipeliness from preprocessing script\n",
    "poly_pipeline = Pipeline([('polynomialfeatures', PolynomialFeatures()), ('linear_regression', LinearRegression())])\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(poly_pipeline)\n",
    "\n",
    "parameters = {\n",
    "    'regressor__polynomialfeatures__degree': [2, 3]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, pipeline, 'neg_root_mean_squared_error', parameters=parameters, data_fraction=0.2)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pipeline, 'neg_root_mean_squared_error', parameters=parameters, data_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "poly_pipeline = Pipeline([('polynomialfeatures', PolynomialFeatures()), ('linear_regression', LinearRegression())])\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(poly_pipeline)\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor(n_neighbors=3))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor(n_neighbors=3))\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.2)\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "parameters = {\n",
    "    'regressor__n_neighbors': range(5,10),\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'strict_columns': ['yardline_100', 'ydstogo', 'score_differential', 'td_prob', 'drive_play_count', 'drive_start_yard_line', 'spread_line', 'total_line', 'overall'],\n",
    "        },\n",
    "        {\n",
    "            'strict_columns': [],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5))\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.05, label=LABEL_RUN)\n",
    "plot_feature_importances(run_pipeline, LABEL_RUN)\n",
    "\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.05, label=LABEL_PASS)\n",
    "plot_feature_importances(run_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__max_depth': range(4, 7),\n",
    "    'regressor__n_estimators': [20, 50, 100]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df)\n",
    "pass_features, pass_target = split_feature_target(pass_df)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree for passes\n",
    "plot_decision_tree(run_pipeline, LABEL_PASS)\n",
    "\n",
    "# Plot the decision tree for runs\n",
    "plot_decision_tree(pass_pipeline, LABEL_RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    xgb.XGBRegressor(\n",
    "        learning_rate = 0.022,\n",
    "        n_estimators  = 1000,\n",
    "        max_depth     = 8,\n",
    "        eval_metric='rmsle'\n",
    "                           )\n",
    ")\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    xgb.XGBRegressor(\n",
    "        learning_rate = 0.015,\n",
    "        n_estimators  = 1000,\n",
    "        max_depth     = 8,\n",
    "        eval_metric='rmsle'\n",
    "        )\n",
    ")\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.05)\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.05)\n",
    "\n",
    "plot_feature_importances(run_pipeline, LABEL_RUN)\n",
    "plot_feature_importances(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "\n",
    "parameters = {\n",
    "    \"regressor__max_depth\":    [8, 10],\n",
    "    \"regressor__n_estimators\": [1000, 1100],\n",
    "    \"regressor__learning_rate\": [0.022, 0.015]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree for passes\n",
    "plot_decision_tree_xgb(run_pipeline, LABEL_PASS)\n",
    "\n",
    "# Plot the decision tree for runs\n",
    "plot_decision_tree_xgb(pass_pipeline, LABEL_RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    MLPWithHistory(\n",
    "        mlp_params={'hidden_layer_sizes': (50,),\n",
    "  'activation': 'relu',\n",
    "  'solver': 'adam',\n",
    "  'max_iter': 100}\n",
    "    )\n",
    ")\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    MLPWithHistory(\n",
    "        mlp_params={'hidden_layer_sizes': (50,),\n",
    "  'activation': 'relu',\n",
    "  'solver': 'adam',\n",
    "  'max_iter': 100}\n",
    "    )\n",
    ")\n",
    "\n",
    "# estimate run model\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.2)\n",
    "run_mlp = run_pipeline.named_steps['regressor']\n",
    "plot_train_val_loss(run_mlp.training_losses, run_mlp.validation_losses, LABEL_RUN)\n",
    "\n",
    "# estimate pass model\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, pass_df, 0.2)\n",
    "pass_mlp = pass_pipeline.named_steps['regressor']\n",
    "plot_train_val_loss(pass_mlp.training_losses, pass_mlp.validation_losses, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__mlp_params': generate_param_combinations({\n",
    "        'hidden_layer_sizes': [(10,), (50,), (10,5), (20,10)], \n",
    "        'activation': ['relu'], \n",
    "        'solver': ['adam'], \n",
    "        'max_iter': [100] \n",
    "    })\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
