{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training for NFL play prediction\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we train multiple regression and ANN models to predict the yards gained per football play. As a basis we use the preselected data created by the `mannheim_meerkats.ipynb`. \n",
    "\n",
    "### Inline preprocessing steps:\n",
    "\n",
    "---\n",
    "\n",
    "As we use 5-fold cross-validation to protect the model against overfitting, we need a dynamic preprocessing appraoch. Therefore a pipeline provided by the `mannheim_meerkats.ipynb` will be used to process the training data of each fold. Further we use a nested cross validation to ensure the quality of our model using selected hyperparameters.\n",
    "\n",
    "\n",
    "### Contributors\n",
    "\n",
    "All contributors are only assigned to their primary task, the teams still interchanged know-how and worked on one anothers approaches.\n",
    "\n",
    "---\n",
    "\n",
    "##### Preprocessing Team\n",
    "\n",
    "- Tim Oliver Krause (1689074)\n",
    "- Jan Thilo Viktorin (1684159)\n",
    "- Joël Pflomm (1634591)\n",
    "\n",
    "##### Model Team\n",
    "\n",
    "- Franziska Köllschen (1981780) \n",
    "- Steffen Hüls (1979863)\n",
    "- Matthias Biermanns (1980701)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, cross_val_predict, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import for preprocessing\n",
    "import preprocessing\n",
    "\n",
    "# import for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# imports for regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# imports for neural network models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-28 00:50:55.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1m--- Executing Preprocessing Steps ---\u001b[0m\n",
      "\u001b[32m2023-11-28 00:50:55.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mmake_combined_df\u001b[0m:\u001b[36m160\u001b[0m - \u001b[1mLoading csv files\u001b[0m\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (36,37,179,180,189,190,197,198,203,204,205,206,213,214,218,219,220,222,224,226,248,249,253,254,255,260,262,263,266,267,268,269,283,284) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,213,214,218,219,220,248,249,253,254,255,260,262,263,266,267,268,269,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,218,219,220,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,189,190,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,189,190,193,194,197,198,203,204,205,206,209,210,213,214,218,219,220,248,249,253,254,255,260,262,263,266,267,268,269,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,207,208,211,212,213,214,218,219,220,248,249,253,254,255,260,262,263,266,267,268,269,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (42,44,45,50,51,52,179,180,182,183,187,188,189,190,193,194,195,196,197,198,201,202,203,204,205,206,207,208,213,214,215,216,217,218,219,220,222,224,226,240,241,242,248,249,250,251,252,253,254,255,256,258,259,260,262,263,264,265,266,267,268,269,277,283,284,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,207,208,218,219,220,248,249,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,218,219,220,248,249,253,254,255,260,262,263,283,284,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,189,190,193,194,197,198,203,204,205,206,213,214,218,219,220,222,224,226,248,249,253,254,255,260,262,263,266,267,268,269,283,284,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,213,214,218,219,220,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,193,194,197,198,203,204,205,206,209,210,218,219,220,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,218,219,220,248,249,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,233,234,235,236,237,238,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,218,219,220,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,213,214,218,219,220,253,254,255,260,262,263,266,267,268,269,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,218,219,220,233,234,235,236,237,238,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,213,214,218,219,220,222,224,226,248,249,253,254,255,260,262,263,266,267,268,269,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,233,234,235,236,237,238,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,222,224,226,243,244,245,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (42,45,179,180,182,183,187,188,203,204,205,206,211,212,213,214,218,219,220,222,224,226,246,247,248,249,253,254,255,260,262,263,264,265,266,267,268,269,277,283,284,332) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "\u001b[32m2023-11-28 00:51:20.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mmake_combined_df\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mSuccessfully loaded csv files\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:20.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mdrop_irrelevant_observations\u001b[0m:\u001b[36m176\u001b[0m - \u001b[1mRemoving irrelevant observations\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mdrop_irrelevant_observations\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mSuccessfully deleted irrelevant observations\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36minsert_missing_values\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mInserting missing values\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36minsert_missing_values\u001b[0m:\u001b[36m241\u001b[0m - \u001b[1mSuccessfully inserted missing values\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mdrop_irrelevant_features\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mDropping irrelevant features\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mdrop_irrelevant_features\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mSuccessfully dropped irrelevant features\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mclear_nas\u001b[0m:\u001b[36m255\u001b[0m - \u001b[1mClaering obervations with NAs\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mclear_nas\u001b[0m:\u001b[36m257\u001b[0m - \u001b[1mSuccessfully cleared observations with NAs\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_run_and_pass_dataframes\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1mSplitting into run and pass dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_run_and_pass_dataframes\u001b[0m:\u001b[36m274\u001b[0m - \u001b[1mSuccessfully split into run and pass dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_test_and_training_dataframes\u001b[0m:\u001b[36m289\u001b[0m - \u001b[1mSplitting into train and test dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_test_and_training_dataframes\u001b[0m:\u001b[36m304\u001b[0m - \u001b[1mSuccessfully split into train and test dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_test_and_training_dataframes\u001b[0m:\u001b[36m289\u001b[0m - \u001b[1mSplitting into train and test dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_test_and_training_dataframes\u001b[0m:\u001b[36m304\u001b[0m - \u001b[1mSuccessfully split into train and test dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mPreparing pipeline\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSuccessfully prepared pipeline\u001b[0m\n",
      "\u001b[32m2023-11-28 00:51:35.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m--- Successfully Loaded Preprocessing Steps ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "FILE_LIST = [f'./Data/play_by_play_{year}.csv' for year in range(1999, 2024)]\n",
    "PREPROCESSOR = preprocessing.NFLPreprocessing(FILE_LIST)\n",
    "TEAM_FEATURES = [\n",
    "        'posteam_ATL', 'posteam_BAL', 'posteam_BUF', 'posteam_CAR', 'posteam_CHI', 'posteam_CIN', 'posteam_CLE', 'posteam_DAL', 'posteam_DEN', 'posteam_DET', 'posteam_GB', \n",
    "        'posteam_HOU', 'posteam_IND', 'posteam_JAX', 'posteam_KC', 'posteam_LA', 'posteam_LAC', 'posteam_LV', 'posteam_MIA', 'posteam_MIN', 'posteam_NE', 'posteam_NO', \n",
    "        'posteam_NYG', 'posteam_NYJ', 'posteam_PHI', 'posteam_PIT', 'posteam_SEA', 'posteam_SF', 'posteam_TB', 'posteam_TEN', 'posteam_WAS', 'posteam_type_home',\n",
    "        'defteam_ATL', 'defteam_BAL', 'defteam_BUF', 'defteam_CAR', 'defteam_CHI', 'defteam_CIN', 'defteam_CLE', 'defteam_DAL', 'defteam_DEN', 'defteam_DET', 'defteam_GB', \n",
    "        'defteam_HOU', 'defteam_IND', 'defteam_JAX', 'defteam_KC', 'defteam_LA', 'defteam_LAC', 'defteam_LV', 'defteam_MIA', 'defteam_MIN', 'defteam_NE', 'defteam_NO', \n",
    "        'defteam_NYG', 'defteam_NYJ', 'defteam_PHI', 'defteam_PIT', 'defteam_SEA', 'defteam_SF', 'defteam_TB', 'defteam_TEN', 'defteam_WAS'\n",
    "    ]\n",
    "TARGET_NAME = 'yards_gained'\n",
    "RANDOM_STATE=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_feature_target(df, data_fraction):\n",
    "    # set fraction between 0 and 1 (e.g. 0.05 -> 5% df)\n",
    "    df_sampled = df.sample(frac=data_fraction, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_sampled = df_sampled.reset_index(drop=True)\n",
    "\n",
    "    features = df_sampled.drop(TARGET_NAME, axis=1)\n",
    "    target = df_sampled[TARGET_NAME]\n",
    "\n",
    "    return features, target\n",
    "\n",
    "def visualize_predicts(y_test, predictions):\n",
    "    # Visualize predictions for passes\n",
    "    plt.scatter(y_test, predictions)\n",
    "    plt.title('Pass Model: True vs Predicted Yards')\n",
    "    plt.xlabel('True Yards')\n",
    "    plt.ylabel('Predicted Yards')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(pipeline, x_train, y_train, x_test, y_test):\n",
    "    pipeline.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = pipeline.predict(x_test)\n",
    "\n",
    "    # Evaluate the models\n",
    "    pass_mse = mean_squared_error(y_test, predictions)\n",
    "    pass_rmse = mean_squared_error(y_test, predictions, squared = False)\n",
    "    pass_msa = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "    print(f\"Mean Squared Error: {pass_mse}\")\n",
    "    print(f\"Mean Absolute Error: {pass_msa}\")\n",
    "    print(f\"Root Mean Squared Error: {pass_rmse}\")\n",
    "    return predictions\n",
    "\n",
    "def test_model_k_fold(df, pipeline, data_fraction, k_folds=5):\n",
    "    features, target = split_feature_target(df, data_fraction)\n",
    "    \n",
    "    cv_results = cross_val_predict(pipeline, features, target, cv=k_folds)\n",
    "    #print(f\"Run Model Cross-Validation Mean Squared Error: {-np.mean(cv_scores)}\")\n",
    "    #print(f\"Run Model Cross-Validation Mean Squared Error: {-np.max(cv_scores)}\")\n",
    "    return cv_results\n",
    "\n",
    "def estimate_hyperparams(df, pipeline, data_fraction, scoring, k_folds=5, parameters={}):\n",
    "    features, target = split_feature_target(df, data_fraction)\n",
    "    \n",
    "    # specify the cross validation\n",
    "    stratified_k_fold_cv = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # create the grid search instance\n",
    "    grid_search_estimator = GridSearchCV(pipeline, parameters, scoring=scoring, cv=stratified_k_fold_cv, return_train_score=False)\n",
    "\n",
    "    # run the grid search\n",
    "    grid_search_estimator.fit(features, target)\n",
    "\n",
    "    return grid_search_estimator\n",
    "\n",
    "def test_model_nested_cv(pipeline, ):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'CAR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\model.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mbier/VSCodeProjects/NFL_play_prediction/model.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m KNeighborsRegressor(\u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mbier/VSCodeProjects/NFL_play_prediction/model.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline([(\u001b[39m'\u001b[39m\u001b[39mscaler\u001b[39m\u001b[39m'\u001b[39m, StandardScaler()), (\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, model)])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mbier/VSCodeProjects/NFL_play_prediction/model.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test_model_k_fold(runs, pipeline, \u001b[39m0.2\u001b[39;49m, \u001b[39m3\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\model.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mbier/VSCodeProjects/NFL_play_prediction/model.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_model_k_fold\u001b[39m(df, pipeline, data_fraction, k_folds\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mbier/VSCodeProjects/NFL_play_prediction/model.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     features, target \u001b[39m=\u001b[39m split_feature_target(df, data_fraction)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mbier/VSCodeProjects/NFL_play_prediction/model.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     cv_results \u001b[39m=\u001b[39m cross_val_predict(pipeline, features, target, cv\u001b[39m=\u001b[39;49mk_folds)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mbier/VSCodeProjects/NFL_play_prediction/model.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m#print(f\"Run Model Cross-Validation Mean Squared Error: {-np.mean(cv_scores)}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mbier/VSCodeProjects/NFL_play_prediction/model.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m#print(f\"Run Model Cross-Validation Mean Squared Error: {-np.max(cv_scores)}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mbier/VSCodeProjects/NFL_play_prediction/model.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cv_results\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:986\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    985\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 986\u001b[0m predictions \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    987\u001b[0m     delayed(_fit_and_predict)(\n\u001b[0;32m    988\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m splits\n\u001b[0;32m    991\u001b[0m )\n\u001b[0;32m    993\u001b[0m inv_test_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(\u001b[39mlen\u001b[39m(test_indices), dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[0;32m    994\u001b[0m inv_test_indices[test_indices] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(test_indices))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1068\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m   1067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1068\u001b[0m     estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m   1069\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(estimator, method)\n\u001b[0;32m   1070\u001b[0m predictions \u001b[39m=\u001b[39m func(X_test)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \n\u001b[0;32m    377\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 401\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[0;32m    402\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    357\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    358\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    360\u001b[0m     cloned_transformer,\n\u001b[0;32m    361\u001b[0m     X,\n\u001b[0;32m    362\u001b[0m     y,\n\u001b[0;32m    363\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    364\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    365\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[0;32m    366\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[0;32m    367\u001b[0m )\n\u001b[0;32m    368\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:862\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 824\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    860\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 861\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    862\u001b[0m     X,\n\u001b[0;32m    863\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    864\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    865\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    866\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    868\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'CAR'"
     ]
    }
   ],
   "source": [
    "runs = PREPROCESSOR.run_df\n",
    "model = KNeighborsRegressor(4)\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "\n",
    "test_model_k_fold(runs, pipeline, 0.2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 800, in transform\n",
      "    Xs = self._fit_transform(\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 876, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 658, in transform\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 877, in transform\n",
      "    X_int, X_mask = self._transform(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 174, in _transform\n",
      "    raise ValueError(msg)\n",
      "ValueError: Found unknown categories ['IND', 'DET', 'CLE'] in column 0 during transform\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 800, in transform\n",
      "    Xs = self._fit_transform(\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 876, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 658, in transform\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 877, in transform\n",
      "    X_int, X_mask = self._transform(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 174, in _transform\n",
      "    raise ValueError(msg)\n",
      "ValueError: Found unknown categories ['LAC', 'NO'] in column 0 during transform\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 800, in transform\n",
      "    Xs = self._fit_transform(\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 876, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 658, in transform\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 877, in transform\n",
      "    X_int, X_mask = self._transform(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 174, in _transform\n",
      "    raise ValueError(msg)\n",
      "ValueError: Found unknown categories ['ARI', 'LV'] in column 0 during transform\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_pipeline = nfl_preprocessing.make_preprocessing_pipeline()\n",
    "\n",
    "run_train= nfl_preprocessing.run_train\n",
    "features = run_train.drop(columns=[\"yards_gained\"])\n",
    "\n",
    "# Extract target variable\n",
    "target = run_train[\"yards_gained\"]\n",
    "\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=4)\n",
    "\n",
    "pre_pipeline.steps.append(('model', model))\n",
    "parameters = {\n",
    "    'model__n_neighbors': range(2, 5)\n",
    "}\n",
    "\n",
    "cross_val_score(pre_pipeline, features, target, cv=3, scoring='neg_mean_squared_error')\n",
    "#feature, target = split_feature_target(nfl_preprocessing.run_df, 0.1)\n",
    "\n",
    "# specify the cross validation\n",
    "#stratified_k_fold_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# create the grid search instance\n",
    "#grid_search_estimator = GridSearchCV(pre_pipeline, parameters, scoring='neg_root_mean_squared_error', cv=stratified_k_fold_cv, return_train_score=False)\n",
    "\n",
    "# run the grid search\n",
    "#grid_search_estimator.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-28 00:39:16.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1m--- Executing Preprocessing Steps ---\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:16.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mmake_combined_df\u001b[0m:\u001b[36m160\u001b[0m - \u001b[1mLoading csv files\u001b[0m\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (36,37,179,180,189,190,197,198,203,204,205,206,213,214,218,219,220,222,224,226,248,249,253,254,255,260,262,263,266,267,268,269,283,284) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,213,214,218,219,220,248,249,253,254,255,260,262,263,266,267,268,269,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,218,219,220,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,189,190,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,189,190,193,194,197,198,203,204,205,206,209,210,213,214,218,219,220,248,249,253,254,255,260,262,263,266,267,268,269,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (37,45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,207,208,211,212,213,214,218,219,220,248,249,253,254,255,260,262,263,266,267,268,269,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (42,44,45,50,51,52,179,180,182,183,187,188,189,190,193,194,195,196,197,198,201,202,203,204,205,206,207,208,213,214,215,216,217,218,219,220,222,224,226,240,241,242,248,249,250,251,252,253,254,255,256,258,259,260,262,263,264,265,266,267,268,269,277,283,284,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,207,208,218,219,220,248,249,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,218,219,220,248,249,253,254,255,260,262,263,283,284,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,189,190,193,194,197,198,203,204,205,206,213,214,218,219,220,222,224,226,248,249,253,254,255,260,262,263,266,267,268,269,283,284,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,213,214,218,219,220,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,193,194,197,198,203,204,205,206,209,210,218,219,220,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,218,219,220,248,249,253,254,255,260,262,263,283,284,294,295,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (45,179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,233,234,235,236,237,238,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,218,219,220,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,213,214,218,219,220,253,254,255,260,262,263,266,267,268,269,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,193,194,197,198,203,204,205,206,218,219,220,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,209,210,218,219,220,233,234,235,236,237,238,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,213,214,218,219,220,222,224,226,248,249,253,254,255,260,262,263,266,267,268,269,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,233,234,235,236,237,238,248,249,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (179,180,182,183,189,190,193,194,197,198,203,204,205,206,218,219,220,222,224,226,243,244,245,253,254,255,260,262,263,283,284,301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "c:\\Users\\mbier\\VSCodeProjects\\NFL_play_prediction\\preprocessing.py:165: DtypeWarning: Columns (42,45,179,180,182,183,187,188,203,204,205,206,211,212,213,214,218,219,220,222,224,226,246,247,248,249,253,254,255,260,262,263,264,265,266,267,268,269,277,283,284,332) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "\u001b[32m2023-11-28 00:39:41.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mmake_combined_df\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mSuccessfully loaded csv files\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:42.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mdrop_irrelevant_observations\u001b[0m:\u001b[36m176\u001b[0m - \u001b[1mRemoving irrelevant observations\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mdrop_irrelevant_observations\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mSuccessfully deleted irrelevant observations\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36minsert_missing_values\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mInserting missing values\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36minsert_missing_values\u001b[0m:\u001b[36m241\u001b[0m - \u001b[1mSuccessfully inserted missing values\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mdrop_irrelevant_features\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mDropping irrelevant features\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mdrop_irrelevant_features\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mSuccessfully dropped irrelevant features\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mclear_nas\u001b[0m:\u001b[36m255\u001b[0m - \u001b[1mClaering obervations with NAs\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36mclear_nas\u001b[0m:\u001b[36m257\u001b[0m - \u001b[1mSuccessfully cleared observations with NAs\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_run_and_pass_dataframes\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1mSplitting into run and pass dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_run_and_pass_dataframes\u001b[0m:\u001b[36m274\u001b[0m - \u001b[1mSuccessfully split into run and pass dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_test_and_training_dataframes\u001b[0m:\u001b[36m289\u001b[0m - \u001b[1mSplitting into train and test dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_test_and_training_dataframes\u001b[0m:\u001b[36m304\u001b[0m - \u001b[1mSuccessfully split into train and test dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_test_and_training_dataframes\u001b[0m:\u001b[36m289\u001b[0m - \u001b[1mSplitting into train and test dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msplit_into_test_and_training_dataframes\u001b[0m:\u001b[36m304\u001b[0m - \u001b[1mSuccessfully split into train and test dataframes\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mPreparing pipeline\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSuccessfully prepared pipeline\u001b[0m\n",
      "\u001b[32m2023-11-28 00:39:56.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m--- Successfully Loaded Preprocessing Steps ---\u001b[0m\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 800, in transform\n",
      "    Xs = self._fit_transform(\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 876, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 658, in transform\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 877, in transform\n",
      "    X_int, X_mask = self._transform(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 174, in _transform\n",
      "    raise ValueError(msg)\n",
      "ValueError: Found unknown categories ['IND', 'DET', 'CLE'] in column 0 during transform\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 800, in transform\n",
      "    Xs = self._fit_transform(\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 876, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 658, in transform\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 877, in transform\n",
      "    X_int, X_mask = self._transform(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 174, in _transform\n",
      "    raise ValueError(msg)\n",
      "ValueError: Found unknown categories ['LAC', 'NO'] in column 0 during transform\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 800, in transform\n",
      "    Xs = self._fit_transform(\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 876, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 658, in transform\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 877, in transform\n",
      "    X_int, X_mask = self._transform(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 174, in _transform\n",
      "    raise ValueError(msg)\n",
      "ValueError: Found unknown categories ['ARI', 'LV'] in column 0 during transform\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_nfl = preprocessing.NFLPreprocessing(FILE_LIST)\n",
    "pipeline = new_nfl.make_preprocessing_pipeline()\n",
    "\n",
    "# fit pipeline\n",
    "run_train= new_nfl.run_train\n",
    "features = run_train.drop(columns=[\"yards_gained\"])\n",
    "\n",
    "# Extract target variable\n",
    "target = run_train[\"yards_gained\"]\n",
    "\n",
    "pipeline.fit(features, target)\n",
    "\n",
    "pipeline.steps.append((\"regression\", LinearRegression()))\n",
    "cross_val_score(pipeline, features, target, cv=3, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data('pass')\n",
    "\n",
    "model = LinearRegression()\n",
    "test_model_k_fold(df, model, 0.1, 'neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data('pass')\n",
    "\n",
    "pipeline = Pipeline([('polynomial', PolynomialFeatures()), ('model', LinearRegression())])\n",
    "#test_model_k_fold(df, pipeline, 0.1, 'neg_root_mean_squared_error')\n",
    "parameters = {\n",
    "    'polynomial__degree': [2, 3]\n",
    "}\n",
    "\n",
    "estimator = estimate_hyperparams(df, pipeline, 0.05, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor()\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "parameters = {\n",
    "    'model__n_neighbors': range(2, 10)\n",
    "}\n",
    "\n",
    "estimator = estimate_hyperparams(df, pipeline, 0.5, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "\n",
    "results = pd.DataFrame(estimator.cv_results_)\n",
    "display(results)\n",
    "display(estimator.best_index_)\n",
    "display(estimator.best_params_)\n",
    "display(estimator.best_score_)\n",
    "display(estimator.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_test, y_train, y_test), (features, target) = prepare_model_data(0.1, True)\n",
    "predictions = estimator.best_estimator_.predict(x_test)\n",
    "\n",
    "visualize_predicts(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest regressor\n",
    "pass_rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5) \n",
    "run_rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5) \n",
    "\n",
    "# Train the model\n",
    "pass_rf_regressor.fit(pass_X_train, pass_y_train)\n",
    "run_rf_regressor.fit(run_X_train, run_y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pass_predictions = pass_rf_regressor.predict(pass_X_test)\n",
    "run_predictions = run_rf_regressor.predict(run_X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "run_mse = mean_squared_error(run_y_test, run_predictions)\n",
    "pass_mse = mean_squared_error(pass_y_test, pass_predictions)\n",
    "\n",
    "run_rmse = mean_squared_error(run_y_test, run_predictions, squared = False)\n",
    "pass_rmse = mean_squared_error(pass_y_test, pass_predictions, squared = False)\n",
    "\n",
    "run_msa = mean_absolute_error(run_y_test, run_predictions)\n",
    "pass_msa = mean_absolute_error(pass_y_test, pass_predictions)\n",
    "\n",
    "print(f\"Run Model Mean Squared Error: {run_mse}\")\n",
    "print(f\"Pass Model Mean Squared Error: {pass_mse}\")\n",
    "print(f\"Run Model Mean Absolute Error: {run_msa}\")\n",
    "print(f\"Pass Model Mean Absolute Error: {pass_msa}\")\n",
    "print(f\"Run Model Root Mean Squared Error: {run_rmse}\")\n",
    "print(f\"Pass Model Root Mean Squared Error: {pass_rmse}\")\n",
    "\n",
    "print(f\"Run Model Mean Squared Error: {run_mse}\")\n",
    "print(f\"Pass Model Mean Squared Error: {pass_mse}\")\n",
    "\n",
    "# Visualize predictions for runs\n",
    "plt.scatter(run_y_test, run_predictions)\n",
    "plt.title('Run Model: True vs Predicted Yards')\n",
    "plt.xlabel('True Yards')\n",
    "plt.ylabel('Predicted Yards')\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions for passes\n",
    "plt.scatter(pass_y_test, pass_predictions)\n",
    "plt.title('Pass Model: True vs Predicted Yards')\n",
    "plt.xlabel('True Yards')\n",
    "plt.ylabel('Predicted Yards')\n",
    "plt.show()\n",
    "\n",
    "# Access a specific tree from the forest (e.g., the first tree)\n",
    "tree_to_plot_pass = 0\n",
    "tree_to_plot_run = 0\n",
    "\n",
    "# Access the decision tree from the Random Forest\n",
    "individual_tree_pass = pass_rf_regressor.estimators_[tree_to_plot_pass]\n",
    "individual_tree_run = run_rf_regressor.estimators_[tree_to_plot_run]\n",
    "\n",
    "# Plot the decision tree for passes\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(individual_tree_pass, filled=True, feature_names=pass_X_train.columns)\n",
    "plt.title('Decision Tree for Pass Model')\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree for runs\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(individual_tree_run, filled=True, feature_names=run_X_train.columns)\n",
    "plt.title('Decision Tree for Run Model')\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation for runs\n",
    "# run_cv_scores = cross_val_score(run_knn_regressor, run_features, run_target, cv=5, scoring='neg_mean_squared_error')\n",
    "# print(f\"Run Model Cross-Validation Mean Squared Error: {-np.mean(run_cv_scores)}\")\n",
    "\n",
    "# Cross-validation for passes\n",
    "# pass_cv_scores = cross_val_score(pass_knn_regressor, pass_features, pass_target, cv=5, scoring='neg_mean_squared_error')\n",
    "# print(f\"Pass Model Cross-Validation Mean Squared Error: {-np.mean(pass_cv_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Standardize the features\n",
    "scaler_run = StandardScaler()\n",
    "X_run_train_scaled = scaler_run.fit_transform(run_X_train)\n",
    "X_run_test_scaled = scaler_run.transform(run_X_test)\n",
    "\n",
    "scaler_pass = StandardScaler()\n",
    "X_pass_train_scaled = scaler_pass.fit_transform(pass_X_train)\n",
    "X_pass_test_scaled = scaler_pass.transform(pass_X_test)\n",
    "\n",
    "# Build and train the neural network for run plays\n",
    "model_run = Sequential()\n",
    "model_run.add(Dense(64, input_dim=X_run_train_scaled.shape[1], activation='relu'))\n",
    "# model_run.add(Dense(32, activation='relu'))\n",
    "model_run.add(Dropout(0.2))\n",
    "model_run.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_run.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "history_run = model_run.fit(X_run_train_scaled, run_y_train, epochs=100, batch_size=32, validation_data=(X_run_test_scaled, run_y_test))\n",
    "\n",
    "# Build and train the neural network for pass plays\n",
    "model_pass = Sequential()\n",
    "model_pass.add(Dense(64, input_dim=X_pass_train_scaled.shape[1], activation='relu'))\n",
    "# model_pass.add(Dense(32, activation='relu'))\n",
    "model_pass.add(Dropout(0.05))\n",
    "model_pass.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_pass.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "history_pass = model_pass.fit(X_pass_train_scaled, pass_y_train, epochs=70, batch_size=32, validation_data=(X_pass_test_scaled, pass_y_test))\n",
    "\n",
    "\n",
    "# Plot the training and validation loss for run plays\n",
    "plt.plot(history_run.history['loss'], label='Train Loss - Run Plays')\n",
    "plt.plot(history_run.history['val_loss'], label='Val Loss - Run Plays')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Neural Network Training and Validation Loss - Run Plays')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss for pass plays\n",
    "plt.plot(history_pass.history['loss'], label='Train Loss - Pass Plays')\n",
    "plt.plot(history_pass.history['val_loss'], label='Val Loss - Pass Plays')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Neural Network Training and Validation Loss - Pass Plays')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Evaluate the neural network for run plays\n",
    "y_run_pred = model_run.predict(X_run_test_scaled)\n",
    "mse_run = mean_squared_error(run_y_test, y_run_pred)\n",
    "print(f\"Mean Squared Error (MSE) for run plays: {mse_run}\")\n",
    "\n",
    "# Evaluate the neural network for pass plays\n",
    "y_pass_pred = model_pass.predict(X_pass_test_scaled)\n",
    "mse_pass = mean_squared_error(pass_y_test, y_pass_pred)\n",
    "print(f\"Mean Squared Error (MSE) for pass plays: {mse_pass}\")\n",
    "\n",
    "# Visualize predictions for run plays\n",
    "plt.scatter(run_y_test, y_run_pred)\n",
    "plt.xlabel('Actual Yards Gained for Run Plays')\n",
    "plt.ylabel('Predicted Yards Gained for Run Plays')\n",
    "plt.title('Neural Network Prediction - Run Plays')\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions for pass plays\n",
    "plt.scatter(pass_y_test, y_pass_pred)\n",
    "plt.xlabel('Actual Yards Gained for Pass Plays')\n",
    "plt.ylabel('Predicted Yards Gained for Pass Plays')\n",
    "plt.title('Neural Network Prediction - Pass Plays')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
