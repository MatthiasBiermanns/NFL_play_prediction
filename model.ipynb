{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training for NFL play prediction\n",
    "\n",
    "---\n",
    " \n",
    "In this notebook, we train multiple regression and ANN models to predict the yards gained per football play. As a basis we use the preselected data created by the `preprocessing.py``.\n",
    "\n",
    "### Inline preprocessing steps:\n",
    "\n",
    "---\n",
    "\n",
    "As we use 5-fold cross-validation to protect the model against overfitting, we need a dynamic preprocessing appraoch. Therefore a pipeline provided by the `preprocessing.py`` will be used to process the training data of each fold. Further we use a nested cross validation to ensure the quality of our model using selected hyperparameters.\n",
    "\n",
    "\n",
    "### Contributors\n",
    "\n",
    "All contributors are only assigned to their primary task, the teams still interchanged know-how and worked on one anothers approaches.\n",
    "\n",
    "---\n",
    "\n",
    "##### Preprocessing Team\n",
    "\n",
    "- Tim Oliver Krause (1689074)\n",
    "- Jan Thilo Viktorin (1684159)\n",
    "- Joël Pflomm (1634591)\n",
    "\n",
    "##### Model Team\n",
    "\n",
    "- Franziska Köllschen (1981780)\n",
    "- Steffen Hüls (1979863)\n",
    "- Matthias Biermanns (1980701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import preprocessing\n",
    "\n",
    "# import for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# imports for regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from subprocess import call\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static variables\n",
    "FILE_LIST = [f'./Data/play_by_play_{year}.csv' for year in range(1999, 2024)]\n",
    "PREPROCESSOR = preprocessing.NFLPreprocessing(FILE_LIST)\n",
    "TARGET_NAME = 'yards_gained'\n",
    "RANDOM_STATE = 42\n",
    "LABEL_PASS = 'Pass'\n",
    "LABEL_RUN = 'Run'\n",
    "\n",
    "# global variables\n",
    "model_counter = 0\n",
    "plot_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(df, data_fraction):\n",
    "    # set fraction between 0 and 1 (e.g. 0.05 -> 5% df)\n",
    "    df_sampled = df.sample(frac=data_fraction, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_sampled = df_sampled.reset_index(drop=True)\n",
    "\n",
    "    return df_sampled\n",
    "\n",
    "def split_feature_target(df):\n",
    "    features = df.drop(TARGET_NAME, axis=1)\n",
    "    target = df[TARGET_NAME]\n",
    "\n",
    "    return features, target\n",
    "\n",
    "def plot_predicts(y_test, predictions, label):\n",
    "    # Evaluate the models\n",
    "    pass_mse = mean_squared_error(y_test, predictions)\n",
    "    pass_rmse = mean_squared_error(y_test, predictions, squared = False)\n",
    "    pass_msa = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "    # Visualize predictions for passes\n",
    "    plt.scatter(y_test, predictions)\n",
    "    plt.title(label + ' Model: True vs Predicted Yards')\n",
    "    plt.xlabel('True Yards')\n",
    "    plt.ylabel('Predicted Yards')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Mean Squared Error: {pass_mse}\")\n",
    "    print(f\"Mean Absolute Error: {pass_msa}\")\n",
    "    print(f\"Root Mean Squared Error: {pass_rmse}\")\n",
    "\n",
    "def plot_decision_tree(pipeline, label, target_tree=0):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_rf'\n",
    "    # Export the decision tree as a dot file\n",
    "    export_graphviz(pipeline.named_steps['regressor'].estimators_[target_tree], out_file=f'{fileName}.dot', \n",
    "                    feature_names=PREPROCESSOR.get_prepro_feature_names_from_pipeline(),\n",
    "                    rounded=True, proportion=False, \n",
    "                    precision=2, filled=True)\n",
    "\n",
    "    # Convert the dot file to png using Graphviz (make sure Graphviz is installed)\n",
    "    call(['dot', '-Tpng', f'{fileName}.dot', '-o', f'{fileName}.png', '-Gdpi=600'])\n",
    "\n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "    display(Image(filename=f'{fileName}.png'))\n",
    "\n",
    "def plot_decision_tree_xgb(pipeline, label, target_tree=0):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_xgb'\n",
    "\n",
    "    xgb.plot_tree(pipeline.named_steps['regressor'], num_trees=target_tree, fmap='featureMap.txt')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(15, 10)\n",
    "    fig.canvas.manager.set_window_title('Decision Tree for ' + label + ' Model')\n",
    "    fig.savefig(f'{fileName}.png')\n",
    "    \n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "    Image(filename=f'{fileName}.png')\n",
    "\n",
    "def plot_train_val_loss(training_losses, validation_losses, label):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_ann'\n",
    "    # Plot the training and validation loss\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Neural Network Training and Validation Loss - ' + label)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{fileName}.png')\n",
    "    plt.show()\n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "def plot_feature_importances(pipeline, label, show_top_n=10):\n",
    "    # Get feature importances\n",
    "    feature_importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "    # Get the feature names after preprocessing\n",
    "    columns = PREPROCESSOR.get_prepro_feature_names_from_pipeline()\n",
    "\n",
    "    # Create a DataFrame to display feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': feature_importances})\n",
    "\n",
    "    # Sort the DataFrame by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df['Feature'][:show_top_n], feature_importance_df['Importance'][:show_top_n])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top {show_top_n} Feature Importances - {label}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_coef(pipeline):\n",
    "    coefs = pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "    # Get the feature names after preprocessing\n",
    "    columns = PREPROCESSOR.get_prepro_feature_names_from_pipeline()\n",
    "\n",
    "    coef = pd.DataFrame(coefs, columns=[\"Coefficients\"], index=columns)\n",
    "    coef.plot(kind=\"barh\", figsize=(9, 7))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(pipeline, df, data_fraction: float = None, label = None):\n",
    "    data = df.copy()\n",
    "    if(data_fraction and data_fraction < 1.0):\n",
    "        data = get_sample(data, data_fraction)\n",
    "    \n",
    "    features, target = split_feature_target(data)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target)\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = pipeline.predict(X_test)\n",
    "\n",
    "    plot_predicts(y_test, predictions, label if label else '')\n",
    "    save_model(features, target, predictions, label, title_appendix=f'normal_{data_fraction*100}%_of_data')\n",
    "\n",
    "    return y_test, predictions\n",
    "\n",
    "def test_model_k_fold(df, pipeline, label, k_folds: int=3, data_fraction: float = 1.0):\n",
    "    data = df.copy()\n",
    "    if(data_fraction and data_fraction < 1.0):\n",
    "        data = get_sample(df, data_fraction)\n",
    "    \n",
    "    features, target = split_feature_target(data)\n",
    "\n",
    "    cv_predictions = cross_val_predict(pipeline, features, target, cv=k_folds)\n",
    "    \n",
    "    plot_predicts(target, cv_predictions, label)\n",
    "    save_model(features, target, cv_predictions, label, title_appendix=f'{k_folds}_folds_{data_fraction*100}%_of_data')\n",
    "    return cv_predictions\n",
    "\n",
    "def estimate_hyperparams(df, pipeline, scoring, label, k_folds=3, parameters={}, data_fraction: float = 1.0):\n",
    "    global model_counter\n",
    "    data = df.copy()\n",
    "    if(data_fraction and data_fraction < 1.0):\n",
    "        data = get_sample(df, data_fraction)\n",
    "\n",
    "    features, target = split_feature_target(data)\n",
    "    \n",
    "    # create the grid search instance\n",
    "    grid_search_estimator = GridSearchCV(pipeline, parameters, scoring=scoring, cv=k_folds, return_train_score=False, n_jobs=1)\n",
    "\n",
    "    # run the grid search\n",
    "    grid_search_estimator.fit(features, target)\n",
    "    \n",
    "    cv_results_df = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "    cv_results_df.to_excel(f'./results/model_{model_counter}_{label}_nestedCV_{data_fraction*100}%_of_data.xlsx')\n",
    "    display(grid_search_estimator.best_params_)\n",
    "    display(cv_results_df)\n",
    "\n",
    "    return grid_search_estimator\n",
    "\n",
    "def generate_param_combinations(parameters):\n",
    "    '''list of list required for cross product of two lists'''\n",
    "    return list(ParameterGrid(parameters))\n",
    "\n",
    "def save_model(features, target, predictions, label, title_appendix: str = ''):\n",
    "    global model_counter\n",
    "\n",
    "    predictions_df = pd.DataFrame({'predicted yards_gained': predictions})\n",
    "    save_model = pd.concat([features, target, predictions_df], axis=1)\n",
    "\n",
    "    if (title_appendix != ''):\n",
    "        title_appendix = '_'+title_appendix\n",
    "    save_model.to_excel(f'./results/model_{model_counter}_{label}{title_appendix}.xlsx')\n",
    "\n",
    "    # increase counter for files\n",
    "    model_counter = model_counter + 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Regressor Class - with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithHistory(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mlp_params=None):\n",
    "        self.mlp_params = mlp_params\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.mlp_regressor = MLPRegressor(**(self.mlp_params or {}))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, X_val, y, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        for epoch in range(self.mlp_regressor.max_iter):\n",
    "            self.mlp_regressor.partial_fit(X, y)\n",
    "\n",
    "            # Calculate training loss\n",
    "            y_train_pred = self.mlp_regressor.predict(X)\n",
    "            training_loss = mean_squared_error(y, y_train_pred)\n",
    "            self.training_losses.append(training_loss)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            y_val_pred = self.mlp_regressor.predict(X_val)\n",
    "            validation_loss = mean_squared_error(y_val, y_val_pred)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.mlp_regressor.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"mlp_params\": self.mlp_params}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.mlp_params = params[\"mlp_params\"]\n",
    "        self.mlp_regressor.set_params(**self.mlp_params)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return -mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_df = PREPROCESSOR.run_df\n",
    "pass_df = PREPROCESSOR.pass_df\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df)\n",
    "pass_features, pass_target = split_feature_target(pass_df)\n",
    "\n",
    "run_X_train, run_X_test, run_y_train, run_y_test = train_test_split(run_features, run_target)\n",
    "pass_X_train, pass_X_test, pass_y_train, pass_y_test = train_test_split(pass_features, pass_target)\n",
    "\n",
    "print(run_features.shape)\n",
    "print(pass_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'strict_factor_iqr': 1.0,\n",
    "        'loose_factor_iqr': 2.0,\n",
    "        'strict_columns': ['yardline_100', 'ydstogo'],\n",
    "        'omit_columns': []\n",
    "    }\n",
    "}\n",
    "\n",
    "pass_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'strict_factor_iqr': 1.5,\n",
    "        'loose_factor_iqr': 3.0,\n",
    "        'strict_columns': ['ydstogo'],\n",
    "        'omit_columns': []\n",
    "    }\n",
    "}\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_y_test, run_predictions = test_model(pipeline, run_df, 0.05, label=LABEL_RUN)\n",
    "plot_coef(pipeline)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_y_test, pass_predictions = test_model(pipeline, pass_df, 0.05, label=LABEL_PASS)\n",
    "plot_coef(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for linear regression\n",
    "\n",
    "# could be more over engineered with automatic dict creation if necessary\n",
    "run_params = {\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'strict_factor_iqr': 1.0,\n",
    "            'loose_factor_iqr': 2.0,\n",
    "            'strict_columns': ['yardline_100', 'ydstogo'],\n",
    "            'omit_columns': []\n",
    "        }, {\n",
    "            'strict_factor_iqr': 1.5,\n",
    "            'loose_factor_iqr': 3.0,\n",
    "            'strict_columns': ['yardline_100', 'ydstogo'],\n",
    "            'omit_columns': []\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "pass_params = {\n",
    "    'outlier_remover__kw_args': [{\n",
    "        'strict_factor_iqr': 1.5,\n",
    "        'loose_factor_iqr': 3.0,\n",
    "        'strict_columns': ['ydstogo'],\n",
    "        'omit_columns': []\n",
    "    }]\n",
    "}\n",
    "\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, pipeline, 'neg_root_mean_squared_error', parameters=run_params, data_fraction=0.25)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pipeline, 'neg_root_mean_squared_error', parameters=pass_params, data_fraction=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic model\n",
    "\n",
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(make_pipeline(PolynomialFeatures(2), LinearRegression()))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(make_pipeline(PolynomialFeatures(2), LinearRegression()))\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.05, label=LABEL_RUN)\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.05, label=LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new pipeliness from preprocessing script\n",
    "poly_pipeline = Pipeline([('polynomialfeatures', PolynomialFeatures()), ('linear_regression', LinearRegression())])\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(poly_pipeline)\n",
    "\n",
    "parameters = {\n",
    "    'regressor__polynomialfeatures__degree': [2, 3]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, pipeline, 'neg_root_mean_squared_error', 'KNN_RUN', parameters=parameters, data_fraction=0.2)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pipeline, 'neg_root_mean_squared_error', 'KNN_RUN', parameters=parameters, data_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "poly_pipeline = Pipeline([('polynomialfeatures', PolynomialFeatures()), ('linear_regression', LinearRegression())])\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(poly_pipeline)\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor(n_neighbors=3))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor(n_neighbors=3))\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.2)\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "parameters = {\n",
    "    'regressor__n_neighbors': range(5,10),\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'strict_columns': ['yardline_100', 'ydstogo', 'score_differential', 'td_prob', 'drive_play_count', 'drive_start_yard_line', 'spread_line', 'total_line', 'overall'],\n",
    "        },\n",
    "        {\n",
    "            'strict_columns': [],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "\n",
    "# run grid search for run and pass with the whole dataset and the hyperparameters specified above\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', 'KNN_RUN', k_folds=3, parameters=parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', 'KNN_RUN', k_folds=3, parameters=parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "\n",
    "# get the best parameters from the hyperparameter tuning\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions for run\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions for pass\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "run_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "pass_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': False,\n",
    "    }\n",
    "}\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5))\n",
    "\n",
    "run_pipeline.set_params(**run_params)\n",
    "pass_pipeline.set_params(**pass_params)\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.05, label=LABEL_RUN)\n",
    "plot_feature_importances(run_pipeline, LABEL_RUN)\n",
    "\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, pass_df, 0.05, label=LABEL_PASS)\n",
    "plot_feature_importances(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__max_depth': range(4, 6),\n",
    "    'regressor__n_estimators': [20],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }, {\n",
    "            'save_stats': True,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', LABEL_RUN, k_folds=3, parameters=parameters, data_fraction=0.05)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', LABEL_PASS, k_folds=3, parameters=parameters, data_fraction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df)\n",
    "pass_features, pass_target = split_feature_target(pass_df)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree for passes\n",
    "plot_decision_tree(run_pipeline, LABEL_PASS)\n",
    "\n",
    "# Plot the decision tree for runs\n",
    "plot_decision_tree(pass_pipeline, LABEL_RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    xgb.XGBRegressor(\n",
    "        learning_rate = 0.022,\n",
    "        n_estimators  = 1000,\n",
    "        max_depth     = 8,\n",
    "        eval_metric='rmsle'\n",
    "                           )\n",
    ")\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    xgb.XGBRegressor(\n",
    "        learning_rate = 0.015,\n",
    "        n_estimators  = 1000,\n",
    "        max_depth     = 8,\n",
    "        eval_metric='rmsle'\n",
    "        )\n",
    ")\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.05)\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.05)\n",
    "\n",
    "# plot the feature importance for run and pass model\n",
    "plot_feature_importances(run_pipeline, LABEL_RUN)\n",
    "plot_feature_importances(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters with testing outlier removals\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "\n",
    "# define the parameters that should be tested\n",
    "parameters = {\n",
    "    \"regressor__max_depth\":    [3, 4, 5],\n",
    "    \"regressor__n_estimators\": [700, 800, 900],\n",
    "    \"regressor__learning_rate\": [0.09, 0.01, 0.011],\n",
    "    \"regressor__gamma\": [0, 5, 10],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'strict_columns': ['yardline_100', 'ydstogo', 'score_differential', 'td_prob', 'drive_play_count', 'drive_start_yard_line', 'spread_line', 'total_line', 'overall'],\n",
    "        },\n",
    "        {\n",
    "            'strict_columns': [],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# run grid search for the defined parameters, first of all with a smaller subset to also test the influence of the outlier removal on the model performance \n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', 'XGB_RUN', k_folds=3, parameters=parameters, data_fraction=0.05)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', 'XGB_RUN', k_folds=3, parameters=parameters, data_fraction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters without testing outlier removals\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "\n",
    "# define the parameters that should be tested\n",
    "parameters = {\n",
    "    \"regressor__max_depth\":    [3, 6, 9],\n",
    "    \"regressor__n_estimators\": [600, 800, 1000],\n",
    "    \"regressor__learning_rate\": [0.09, 0.01, 0.011],\n",
    "    \"regressor__gamma\": [0, 5, 10]\n",
    "}\n",
    "\n",
    "#run grid search now only with no columns set to strict for the outlier removal as that had given the better results above\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', 'XGB_RUN', k_folds=3, parameters=parameters, data_fraction=0.25)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', 'XGB_RUN', k_folds=3, parameters=parameters, data_fraction=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training & testing the model with the best combination of hyperparameters using a 3 fold cross validation\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "\n",
    "# extracting the best hyperparameter set from the grid search done previously\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model, save predictions and plot the feature importance\n",
    "run_pipeline = run_pipeline.set_params(**run_params)\n",
    "run_predictions = test_model(run_pipeline, run_df, 1.0)\n",
    "plot_feature_importances(run_pipeline, LABEL_RUN)\n",
    "\n",
    "# set params, test model, save predictions and plot the feature importance\n",
    "pass_pipeline = pass_pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model(pass_pipeline, pass_df, 1.0)\n",
    "plot_feature_importances(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting more insights into the models\n",
    "\n",
    "# Plot a decision tree for passes\n",
    "plot_decision_tree_xgb(run_pipeline, LABEL_PASS)\n",
    "\n",
    "# Plot a decision tree for runs\n",
    "plot_decision_tree_xgb(pass_pipeline, LABEL_RUN)\n",
    "\n",
    "# plot the feature importance of the pass model by using the weight, gain and the cover \n",
    "xgb.plot_importance(pass_pipeline.named_steps['regressor'], importance_type=\"weight\", fmap='featureMap.txt')\n",
    "xgb.plot_importance(pass_pipeline.named_steps['regressor'], importance_type=\"gain\", fmap='featureMap.txt')\n",
    "xgb.plot_importance(pass_pipeline.named_steps['regressor'], importance_type=\"cover\", fmap='featureMap.txt')\n",
    "\n",
    "# plot the feature importance of the run model by using the weight, gain and the cover \n",
    "xgb.plot_importance(run_pipeline.named_steps['regressor'], importance_type=\"weight\", fmap='featureMap.txt')\n",
    "xgb.plot_importance(run_pipeline.named_steps['regressor'], importance_type=\"gain\", fmap='featureMap.txt')\n",
    "xgb.plot_importance(run_pipeline.named_steps['regressor'], importance_type=\"cover\", fmap='featureMap.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    MLPWithHistory(\n",
    "        mlp_params={'hidden_layer_sizes': (50,),\n",
    "  'activation': 'relu',\n",
    "  'solver': 'adam',\n",
    "  'max_iter': 100}\n",
    "    )\n",
    ")\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    MLPWithHistory(\n",
    "        mlp_params={'hidden_layer_sizes': (50,),\n",
    "  'activation': 'relu',\n",
    "  'solver': 'adam',\n",
    "  'max_iter': 100}\n",
    "    )\n",
    ")\n",
    "\n",
    "# estimate run model\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.2)\n",
    "run_mlp = run_pipeline.named_steps['regressor']\n",
    "plot_train_val_loss(run_mlp.training_losses, run_mlp.validation_losses, LABEL_RUN)\n",
    "\n",
    "# estimate pass model\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, pass_df, 0.2)\n",
    "pass_mlp = pass_pipeline.named_steps['regressor']\n",
    "plot_train_val_loss(pass_mlp.training_losses, pass_mlp.validation_losses, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__mlp_params': generate_param_combinations({\n",
    "        'hidden_layer_sizes': [(10,), (50,), (10,5), (20,10)], \n",
    "        'activation': ['relu'], \n",
    "        'solver': ['adam'], \n",
    "        'max_iter': [100] \n",
    "    })\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
