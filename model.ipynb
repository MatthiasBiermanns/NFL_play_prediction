{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training for NFL play prediction\n",
    "\n",
    "---\n",
    " \n",
    "In this notebook, we train multiple regression and ANN models to predict the yards gained per football play. As a basis we use the preselected data created by the `preprocessing.py``.\n",
    "\n",
    "### Inline preprocessing steps:\n",
    "\n",
    "---\n",
    "\n",
    "As we use 5-fold cross-validation to protect the model against overfitting, we need a dynamic preprocessing appraoch. Therefore a pipeline provided by the `preprocessing.py`` will be used to process the training data of each fold. Further we use a nested cross validation to ensure the quality of our model using selected hyperparameters.\n",
    "\n",
    "\n",
    "### Contributors\n",
    "\n",
    "All contributors are only assigned to their primary task, the teams still interchanged know-how and worked on one anothers approaches.\n",
    "\n",
    "---\n",
    "\n",
    "##### Preprocessing Team\n",
    "\n",
    "- Tim Oliver Krause (1689074)\n",
    "- Jan Thilo Viktorin (1684159)\n",
    "- Joël Pflomm (1634591)\n",
    "\n",
    "##### Model Team\n",
    "\n",
    "- Franziska Köllschen (1981780)\n",
    "- Steffen Hüls (1979863)\n",
    "- Matthias Biermanns (1980701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import preprocessing\n",
    "\n",
    "# import for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# imports for regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static variables\n",
    "FILE_LIST = [f'./Data/play_by_play_{year}.csv' for year in range(1999, 2024)]\n",
    "PREPROCESSOR = preprocessing.NFLPreprocessing(FILE_LIST)\n",
    "TARGET_NAME = 'yards_gained'\n",
    "RANDOM_STATE = 42\n",
    "LABEL_PASS = 'Pass'\n",
    "LABEL_RUN = 'Run'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_feature_target(df, data_fraction=1):\n",
    "    # set fraction between 0 and 1 (e.g. 0.05 -> 5% df)\n",
    "    df_sampled = df.sample(frac=data_fraction, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_sampled = df_sampled.reset_index(drop=True)\n",
    "\n",
    "    features = df_sampled.drop(TARGET_NAME, axis=1)\n",
    "    target = df_sampled[TARGET_NAME]\n",
    "\n",
    "    return features, target\n",
    "\n",
    "def visualize_predicts(y_test, predictions, label):\n",
    "    # Visualize predictions for passes\n",
    "    plt.scatter(y_test, predictions)\n",
    "    plt.title(label + ' Model: True vs Predicted Yards')\n",
    "    plt.xlabel('True Yards')\n",
    "    plt.ylabel('Predicted Yards')\n",
    "    plt.show()\n",
    "\n",
    "def plot_decision_tree(tree, label):\n",
    "    # Plot the decision tree for runs\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plot_tree(tree, filled=True, feature_names=PREPROCESSOR.get_prepro_feature_names_from_pipeline())\n",
    "    plt.title('Decision Tree for ' + label + ' Model')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_train_val_loss(training_losses, validation_losses, label):\n",
    "    # Plot the training and validation loss\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Neural Network Training and Validation Loss - ' + label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importances(pipeline, show_top_n=10):\n",
    "    # Get feature importances\n",
    "    feature_importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "    # Get the feature names after preprocessing\n",
    "    columns = PREPROCESSOR.get_prepro_feature_names_from_pipeline()\n",
    "\n",
    "    # Create a DataFrame to display feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': feature_importances})\n",
    "\n",
    "    # Sort the DataFrame by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df['Feature'][:show_top_n], feature_importance_df['Importance'][:show_top_n])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top {} Feature Importances'.format(show_top_n))\n",
    "    plt.show()\n",
    "\n",
    "def plot_coef(pipeline):\n",
    "    coefs = pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "    # Get the feature names after preprocessing\n",
    "    columns = PREPROCESSOR.get_prepro_feature_names_from_pipeline()\n",
    "\n",
    "    coef = pd.DataFrame(coefs, columns=[\"Coefficients\"], index=columns)\n",
    "    coef.plot(kind=\"barh\", figsize=(9, 7))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Ridge model\")\n",
    "    plt.axvline(x=0, color=\".5\")\n",
    "    plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(pipeline, x_train, y_train, x_test, y_test):\n",
    "    pipeline.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = pipeline.predict(x_test)\n",
    "\n",
    "    # Evaluate the models\n",
    "    pass_mse = mean_squared_error(y_test, predictions)\n",
    "    pass_rmse = mean_squared_error(y_test, predictions, squared = False)\n",
    "    pass_msa = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "    print(f\"Mean Squared Error: {pass_mse}\")\n",
    "    print(f\"Mean Absolute Error: {pass_msa}\")\n",
    "    print(f\"Root Mean Squared Error: {pass_rmse}\")\n",
    "    return predictions\n",
    "\n",
    "def test_model_k_fold(df, pipeline, data_fraction, k_folds=5):\n",
    "    features, target = split_feature_target(df, data_fraction)\n",
    "    \n",
    "    cv_results = cross_val_predict(pipeline, features, target, cv=k_folds)\n",
    "    #print(f\"Run Model Cross-Validation Mean Squared Error: {np.mean(cv_results)}\")\n",
    "    #print(f\"Run Model Cross-Validation Max Squared Error: {np.max(cv_results)}\")\n",
    "    return cv_results\n",
    "\n",
    "def estimate_hyperparams(features, target, pipeline, scoring, k_folds=5, parameters={}):\n",
    "    # create the grid search instance\n",
    "    grid_search_estimator = GridSearchCV(pipeline, parameters, scoring=scoring, cv=k_folds, return_train_score=False)\n",
    "\n",
    "    # run the grid search\n",
    "    grid_search_estimator.fit(features, target)\n",
    "\n",
    "    return grid_search_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Regressor Class - with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithHistory(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mlp_params=None):\n",
    "        self.mlp_params = mlp_params\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.mlp_regressor = MLPRegressor(**(self.mlp_params or {}))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, X_val, y, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        for epoch in range(self.mlp_regressor.max_iter):\n",
    "            self.mlp_regressor.partial_fit(X, y)\n",
    "\n",
    "            # Calculate training loss\n",
    "            y_train_pred = self.mlp_regressor.predict(X)\n",
    "            training_loss = mean_squared_error(y, y_train_pred)\n",
    "            self.training_losses.append(training_loss)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            y_val_pred = self.mlp_regressor.predict(X_val)\n",
    "            validation_loss = mean_squared_error(y_val, y_val_pred)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.mlp_regressor.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"mlp_params\": self.mlp_params}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.mlp_params = params[\"mlp_params\"]\n",
    "        self.mlp_regressor.set_params(**self.mlp_params)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return -mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_df = PREPROCESSOR.run_df\n",
    "pass_df = PREPROCESSOR.pass_df\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df, 0.2)\n",
    "pass_features, pass_target = split_feature_target(pass_df, 0.2)\n",
    "\n",
    "run_X_train, run_X_test, run_y_train, run_y_test = train_test_split(run_features, run_target)\n",
    "pass_X_train, pass_X_test, pass_y_train, pass_y_test = train_test_split(pass_features, pass_target)\n",
    "\n",
    "print(run_features.shape)\n",
    "print(pass_features.shape)\n",
    "\n",
    "run_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline()\n",
    "\n",
    "run_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'strict_factor_iqr': 1.0,\n",
    "        'loose_factor_iqr': 2.0,\n",
    "        'strict_columns': ['yardline_100', 'ydstogo'],\n",
    "        'omit_columns': []\n",
    "    }\n",
    "}\n",
    "\n",
    "pass_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'strict_factor_iqr': 1.5,\n",
    "        'loose_factor_iqr': 3.0,\n",
    "        'strict_columns': ['ydstogo'],\n",
    "        'omit_columns': []\n",
    "    }\n",
    "}\n",
    "\n",
    "# add model to pipeline\n",
    "# pipeline.steps.append((\"regressor\", LinearRegression()))\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model(pipeline, run_X_train, run_y_train, run_X_test, run_y_test)\n",
    "\n",
    "plot_coef(pipeline)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model(pipeline, pass_X_train, pass_y_train, pass_X_test, pass_y_test)\n",
    "\n",
    "# visualize predictions\n",
    "visualize_predicts(run_y_test, run_predictions, LABEL_RUN)\n",
    "visualize_predicts(pass_y_test, pass_predictions, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_cv_scores = cross_val_score(pipeline, run_features, run_target, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Run Model Cross-Validation Mean Squared Error: {-np.mean(run_cv_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for linear regression\n",
    "\n",
    "# could be more over engineered with automatic dict creation if necessary\n",
    "run_params = {\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'strict_factor_iqr': 1.0,\n",
    "            'loose_factor_iqr': 2.0,\n",
    "            'strict_columns': ['yardline_100', 'ydstogo'],\n",
    "            'omit_columns': []\n",
    "        }, {\n",
    "            'strict_factor_iqr': 1.5,\n",
    "            'loose_factor_iqr': 3.0,\n",
    "            'strict_columns': ['yardline_100', 'ydstogo'],\n",
    "            'omit_columns': []\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "pass_params = {\n",
    "    'outlier_remover__kw_args': [{\n",
    "        'strict_factor_iqr': 1.5,\n",
    "        'loose_factor_iqr': 3.0,\n",
    "        'strict_columns': ['ydstogo'],\n",
    "        'omit_columns': []\n",
    "    }]\n",
    "}\n",
    "\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_features, run_target, pipeline, scoring='neg_mean_squared_error', parameters=run_params)\n",
    "pass_grid_search = estimate_hyperparams(pass_features, pass_target, pipeline, scoring='neg_mean_squared_error', parameters=pass_params)\n",
    "\n",
    "display(run_grid_search.best_params_)\n",
    "display(pd.DataFrame(run_grid_search.cv_results_))\n",
    "display(pass_grid_search.best_params_)\n",
    "display(pd.DataFrame(pass_grid_search.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model(pipeline, run_X_train, run_y_train, run_X_test, run_y_test)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model(pipeline, pass_X_train, pass_y_train, pass_X_test, pass_y_test)\n",
    "\n",
    "# visualize predictions\n",
    "visualize_predicts(run_y_test, run_predictions, LABEL_RUN)\n",
    "visualize_predicts(pass_y_test, pass_predictions, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic model\n",
    "\n",
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(make_pipeline(PolynomialFeatures(2), LinearRegression()))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(make_pipeline(PolynomialFeatures(2), LinearRegression()))\n",
    "\n",
    "# test model and save predictions\n",
    "run_predictions = test_model(run_pipeline, run_X_train, run_y_train, run_X_test, run_y_test)\n",
    "pass_predictions = test_model(pass_pipeline, pass_X_train, pass_y_train, pass_X_test, pass_y_test)\n",
    "\n",
    "# visualize predictions\n",
    "visualize_predicts(run_y_test, run_predictions, LABEL_RUN)\n",
    "visualize_predicts(pass_y_test, pass_predictions, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df, 0.01)\n",
    "pass_features, pass_target = split_feature_target(pass_df, 0.01)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(make_pipeline(PolynomialFeatures(), LinearRegression()))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(make_pipeline(PolynomialFeatures(), LinearRegression()))\n",
    "\n",
    "parameters = {\n",
    "    'regressor__polynomialfeatures__degree': [2, 3]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_features, run_target, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "pass_grid_search = estimate_hyperparams(pass_features, pass_target, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "\n",
    "display(run_grid_search.best_params_)\n",
    "display(pass_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df, 0.01)\n",
    "pass_features, pass_target = split_feature_target(pass_df, 0.01)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(make_pipeline(PolynomialFeatures(), LinearRegression()))\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_cv_scores = cross_val_score(pipeline, run_features, run_target, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Run Model Cross-Validation Mean Squared Error: {-np.mean(run_cv_scores)}\")\n",
    "\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_cv_scores = cross_val_score(pipeline, pass_features, pass_target, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Pass Model Cross-Validation Mean Squared Error: {-np.mean(pass_cv_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor(n_neighbors=3))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor(n_neighbors=3))\n",
    "\n",
    "# test model and save predictions\n",
    "run_predictions = test_model(run_pipeline, run_X_train, run_y_train, run_X_test, run_y_test)\n",
    "pass_predictions = test_model(pass_pipeline, pass_X_train, pass_y_train, pass_X_test, pass_y_test)\n",
    "\n",
    "# visualize predictions\n",
    "visualize_predicts(run_y_test, run_predictions, LABEL_RUN)\n",
    "visualize_predicts(pass_y_test, pass_predictions, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df, 0.1)\n",
    "pass_features, pass_target = split_feature_target(pass_df, 0.1)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__n_neighbors': range(1, 10)\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_features, run_target, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "pass_grid_search = estimate_hyperparams(pass_features, pass_target, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "\n",
    "display(run_grid_search.best_params_)\n",
    "display(pass_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df)\n",
    "pass_features, pass_target = split_feature_target(pass_df)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_cv_scores = cross_val_score(run_pipeline, run_features, run_target, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Run Model Cross-Validation Mean Squared Error: {-np.mean(run_cv_scores)}\")\n",
    "\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_cv_scores = cross_val_score(pass_pipeline, pass_features, pass_target, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Pass Model Cross-Validation Mean Squared Error: {-np.mean(pass_cv_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5))\n",
    "\n",
    "# test model and save predictions\n",
    "run_predictions = test_model(run_pipeline, run_X_train, run_y_train, run_X_test, run_y_test)\n",
    "plot_feature_importances(run_pipeline)\n",
    "\n",
    "pass_predictions = test_model(pass_pipeline, pass_X_train, pass_y_train, pass_X_test, pass_y_test)\n",
    "plot_feature_importances(run_pipeline)\n",
    "\n",
    "# visualize predictions\n",
    "visualize_predicts(run_y_test, run_predictions, LABEL_RUN)\n",
    "visualize_predicts(pass_y_test, pass_predictions, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df, 0.01)\n",
    "pass_features, pass_target = split_feature_target(pass_df, 0.01)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__max_depth': range(4, 7),\n",
    "    'regressor__n_estimators': [20, 50, 100]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_features, run_target, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "pass_grid_search = estimate_hyperparams(pass_features, pass_target, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "\n",
    "display(run_grid_search.best_params_)\n",
    "display(pass_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df)\n",
    "pass_features, pass_target = split_feature_target(pass_df)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_cv_scores = cross_val_score(pipeline, run_features, run_target, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Run Model Cross-Validation Mean Squared Error: {-np.mean(run_cv_scores)}\")\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_cv_scores = cross_val_score(pipeline, pass_features, pass_target, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Pass Model Cross-Validation Mean Squared Error: {-np.mean(pass_cv_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize decision tree\n",
    "# test model\n",
    "test_model(run_pipeline, run_X_train, run_y_train, run_X_test, run_y_test)\n",
    "test_model(pass_pipeline, pass_X_train, pass_y_train, pass_X_test, pass_y_test)\n",
    "\n",
    "# Access a specific tree from the forest (e.g., the first tree)\n",
    "tree_to_plot_pass = 0\n",
    "tree_to_plot_run = 0\n",
    "\n",
    "pass_rf_regressor = pass_pipeline.named_steps['regressor']\n",
    "run_rf_regressor = run_pipeline.named_steps['regressor']\n",
    "\n",
    "# Access the decision tree from the Random Forest\n",
    "individual_tree_pass = pass_rf_regressor.estimators_[tree_to_plot_pass]\n",
    "individual_tree_run = run_rf_regressor.estimators_[tree_to_plot_run]\n",
    "\n",
    "# Plot the decision tree for passes\n",
    "plot_decision_tree(individual_tree_pass, LABEL_PASS)\n",
    "\n",
    "# Plot the decision tree for runs\n",
    "plot_decision_tree(individual_tree_run, LABEL_RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    xgb.XGBRegressor(\n",
    "        learning_rate = 0.022,\n",
    "        n_estimators  = 1000,\n",
    "        max_depth     = 8,\n",
    "        eval_metric='rmsle'\n",
    "                           )\n",
    ")\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    xgb.XGBRegressor(\n",
    "        learning_rate = 0.015,\n",
    "        n_estimators  = 1000,\n",
    "        max_depth     = 8,\n",
    "        eval_metric='rmsle'\n",
    "        )\n",
    ")\n",
    "\n",
    "# test model and save predictions\n",
    "run_predictions = test_model(run_pipeline, run_X_train, run_y_train, run_X_test, run_y_test)\n",
    "pass_predictions = test_model(pass_pipeline, pass_X_train, pass_y_train, pass_X_test, pass_y_test)\n",
    "\n",
    "plot_feature_importances(run_pipeline)\n",
    "plot_feature_importances(pass_pipeline)\n",
    "\n",
    "# visualize predictions\n",
    "visualize_predicts(run_y_test, run_predictions, LABEL_RUN)\n",
    "visualize_predicts(pass_y_test, pass_predictions, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df, 0.05)\n",
    "pass_features, pass_target = split_feature_target(pass_df, 0.05)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "\n",
    "parameters = {\n",
    "    \"regressor__max_depth\":    [8, 10],\n",
    "    \"regressor__n_estimators\": [1000, 1100],\n",
    "    \"regressor__learning_rate\": [0.022, 0.015]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_features, run_target, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "pass_grid_search = estimate_hyperparams(pass_features, pass_target, pass_pipeline, 0.05, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "\n",
    "display(run_grid_search.best_params_)\n",
    "display(pass_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    MLPWithHistory(\n",
    "        mlp_params={'hidden_layer_sizes': (50,),\n",
    "  'activation': 'relu',\n",
    "  'solver': 'adam',\n",
    "  'max_iter': 100}\n",
    "    )\n",
    ")\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    MLPWithHistory(\n",
    "        mlp_params={'hidden_layer_sizes': (50,),\n",
    "  'activation': 'relu',\n",
    "  'solver': 'adam',\n",
    "  'max_iter': 100}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit the pipelines\n",
    "run_pipeline.fit(run_X_train, run_y_train)\n",
    "pass_pipeline.fit(pass_X_train, pass_y_train)\n",
    "\n",
    "run_mlp = run_pipeline.named_steps['regressor']\n",
    "pass_mlp = pass_pipeline.named_steps['regressor']\n",
    "\n",
    "# Plot the training and validation loss\n",
    "visualize_train_val_loss(run_mlp.training_losses, run_mlp.validation_losses, LABEL_RUN)\n",
    "visualize_train_val_loss(pass_mlp.training_losses, pass_mlp.validation_losses, LABEL_PASS)\n",
    "\n",
    "# Evaluate the neural network for run plays\n",
    "y_run_pred = run_pipeline.predict(run_X_test)\n",
    "mse_run = mean_squared_error(run_y_test, y_run_pred)\n",
    "print(f\"Mean Squared Error (MSE) for run plays: {mse_run}\")\n",
    "\n",
    "# Evaluate the neural network for pass plays\n",
    "y_pass_pred = pass_pipeline.predict(pass_X_test)\n",
    "mse_pass = mean_squared_error(pass_y_test, y_pass_pred)\n",
    "print(f\"Mean Squared Error (MSE) for pass plays: {mse_pass}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df, 0.01)\n",
    "pass_features, pass_target = split_feature_target(pass_df, 0.01)\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__mlp_params': [\n",
    "        {'hidden_layer_sizes': (10,), 'activation': 'relu', 'solver': 'adam', 'max_iter': 100},\n",
    "        {'hidden_layer_sizes': (50,), 'activation': 'relu', 'solver': 'adam', 'max_iter': 100},\n",
    "        {'hidden_layer_sizes': (10, 5), 'activation': 'relu', 'solver': 'adam', 'max_iter': 100},\n",
    "        {'hidden_layer_sizes': (20, 10), 'activation': 'relu', 'solver': 'adam', 'max_iter': 100},\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_estimator = estimate_hyperparams(run_features, run_target, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "pass_estimator = estimate_hyperparams(pass_features, pass_target, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters)\n",
    "\n",
    "display(run_estimator.best_params_)\n",
    "display(pd.DataFrame(run_estimator.cv_results_))\n",
    "display(pass_estimator.best_params_)\n",
    "display(pd.DataFrame(pass_estimator.cv_results_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
