{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training for NFL play prediction\n",
    "\n",
    "---\n",
    " \n",
    "In this notebook, we train multiple regression and ANN models to predict the yards gained per football play. As a basis we use the preselected data created by the `preprocessing.py``.\n",
    "\n",
    "### Inline preprocessing steps:\n",
    "\n",
    "---\n",
    "\n",
    "As we use 5-fold cross-validation to protect the model against overfitting, we need a dynamic preprocessing appraoch. Therefore a pipeline provided by the `preprocessing.py`` will be used to process the training data of each fold. Further we use a nested cross validation to ensure the quality of our model using selected hyperparameters.\n",
    "\n",
    "\n",
    "### Contributors\n",
    "\n",
    "All contributors are only assigned to their primary task, the teams still interchanged know-how and worked on one anothers approaches.\n",
    "\n",
    "---\n",
    "\n",
    "##### Preprocessing Team\n",
    "\n",
    "- Tim Oliver Krause (1689074)\n",
    "- Jan Thilo Viktorin (1684159)\n",
    "- Joël Pflomm (1634591)\n",
    "\n",
    "##### Model Team\n",
    "\n",
    "- Franziska Köllschen (1981780)\n",
    "- Steffen Hüls (1979863)\n",
    "- Matthias Biermanns (1980701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import preprocessing\n",
    "\n",
    "# import for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# imports for regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from subprocess import call\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static variables\n",
    "FILE_LIST = [f'./Data/play_by_play_{year}.csv' for year in range(1999, 2024)]\n",
    "PREPROCESSOR = preprocessing.NFLPreprocessing(FILE_LIST)\n",
    "TARGET_NAME = 'yards_gained'\n",
    "PRED_SAVE_NAME = 'predicted yards_gained'\n",
    "RANDOM_STATE = 42\n",
    "LABEL_PASS = 'Pass'\n",
    "LABEL_RUN = 'Run'\n",
    "\n",
    "# global variables\n",
    "model_counter = 0\n",
    "plot_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(df, data_fraction):\n",
    "    # set fraction between 0 and 1 (e.g. 0.05 -> 5% df)\n",
    "    df_sampled = df.sample(frac=data_fraction, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_sampled = df_sampled.reset_index(drop=True)\n",
    "\n",
    "    return df_sampled\n",
    "\n",
    "def split_feature_target(df):\n",
    "    features = df.drop(TARGET_NAME, axis=1)\n",
    "    target = df[TARGET_NAME]\n",
    "\n",
    "    return features, target\n",
    "\n",
    "def plot_predicts(y_test, predictions, label):\n",
    "    # Evaluate the models\n",
    "    pass_mse = mean_squared_error(y_test, predictions)\n",
    "    pass_rmse = mean_squared_error(y_test, predictions, squared = False)\n",
    "    pass_msa = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "    # Visualize predictions for passes\n",
    "    plt.scatter(y_test, predictions)\n",
    "    plt.title(label + ' Model: True vs Predicted Yards')\n",
    "    plt.xlabel('True Yards')\n",
    "    plt.ylabel('Predicted Yards')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Mean Squared Error: {pass_mse}\")\n",
    "    print(f\"Mean Absolute Error: {pass_msa}\")\n",
    "    print(f\"Root Mean Squared Error: {pass_rmse}\")\n",
    "\n",
    "def plot_decision_tree(pipeline, label, target_tree=0):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_rf'\n",
    "    # Export the decision tree as a dot file\n",
    "    export_graphviz(pipeline.named_steps['regressor'].estimators_[target_tree], out_file=f'{fileName}.dot', \n",
    "                    feature_names=PREPROCESSOR.get_prepro_feature_names_from_pipeline(),\n",
    "                    rounded=True, proportion=False, \n",
    "                    precision=2, filled=True)\n",
    "\n",
    "    # Convert the dot file to png using Graphviz (make sure Graphviz is installed)\n",
    "    call(['dot', '-Tpng', f'{fileName}.dot', '-o', f'{fileName}.png', '-Gdpi=600'])\n",
    "\n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "    display(Image(filename=f'{fileName}.png'))\n",
    "\n",
    "def plot_decision_tree_xgb(pipeline, label, target_tree=0):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_xgb'\n",
    "\n",
    "    xgb.plot_tree(pipeline.named_steps['regressor'], num_trees=target_tree)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(15, 10)\n",
    "    fig.canvas.manager.set_window_title('Decision Tree for ' + label + ' Model')\n",
    "    fig.savefig(f'{fileName}.png')\n",
    "    \n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "    Image(filename=f'{fileName}.png')\n",
    "\n",
    "def plot_train_val_loss(training_losses, validation_losses, label):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_ann'\n",
    "    # Plot the training and validation loss\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Neural Network Training and Validation Loss - ' + label)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{fileName}.png')\n",
    "    plt.show()\n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "def plot_feature_importances(pipeline, label, show_top_n=10):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_feature_importance'\n",
    "    # Get feature importances\n",
    "    feature_importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "    # Get the feature names after preprocessing\n",
    "    columns = PREPROCESSOR.get_prepro_feature_names_from_pipeline()\n",
    "\n",
    "    # Create a DataFrame to display feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': feature_importances})\n",
    "\n",
    "    # Sort the DataFrame by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df['Feature'][:show_top_n], feature_importance_df['Importance'][:show_top_n])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top {show_top_n} Feature Importances - {label}')\n",
    "    plt.savefig(f'{fileName}.png')\n",
    "    plt.show()\n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "def plot_coef(pipeline, label):\n",
    "    global plot_counter\n",
    "    fileName = f'./results/plot_{plot_counter}_{label}_feature_importance'\n",
    "    coefs = pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "    # Get the feature names after preprocessing\n",
    "    columns = PREPROCESSOR.get_prepro_feature_names_from_pipeline()\n",
    "\n",
    "    coef = pd.DataFrame(coefs, columns=[\"Coefficients\"], index=columns)\n",
    "    coef.plot(kind=\"barh\", figsize=(9, 7))\n",
    "    plt.savefig(f'{fileName}.png')\n",
    "    plt.show()\n",
    "    plot_counter = plot_counter + 1\n",
    "\n",
    "def estimate_model_from_excel(path: str):\n",
    "    df = pd.read_excel(path)\n",
    "    df = df.dropna()\n",
    "    #df = df[(df[TARGET_NAME] != np.NaN) & (df[PRED_SAVE_NAME] != np.NaN)]\n",
    "    label = path.split('_')[2]\n",
    "\n",
    "    target = df[TARGET_NAME]\n",
    "    predictions = df[PRED_SAVE_NAME]\n",
    "\n",
    "    plot_predicts(target, predictions, label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_model_from_excel('./results/save_LinearRegression/save_Lasso/final/model_18_Run_3_folds_100.0%_of_data.xlsx')\n",
    "estimate_model_from_excel('./results/save_LinearRegression/save_Lasso/final/model_19_Pass_3_folds_100.0%_of_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(pipeline, df, data_fraction: float = None, label = None):\n",
    "    data = df.copy()\n",
    "    if(data_fraction and data_fraction < 1.0):\n",
    "        data = get_sample(data, data_fraction)\n",
    "    \n",
    "    features, target = split_feature_target(data)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target)\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = pipeline.predict(X_test)\n",
    "\n",
    "    plot_predicts(y_test, predictions, label if label else '')\n",
    "    save_model(features, target, predictions, label, title_appendix=f'normal_{data_fraction*100}%_of_data')\n",
    "\n",
    "    return y_test, predictions\n",
    "\n",
    "def test_model_k_fold(df, pipeline, label, k_folds: int=3, data_fraction: float = 1.0):\n",
    "    data = df.copy()\n",
    "    if(data_fraction and data_fraction < 1.0):\n",
    "        data = get_sample(df, data_fraction)\n",
    "    \n",
    "    features, target = split_feature_target(data)\n",
    "\n",
    "    cv_predictions = cross_val_predict(pipeline, features, target, cv=k_folds)\n",
    "    \n",
    "    plot_predicts(target, cv_predictions, label)\n",
    "    save_model(features, target, cv_predictions, label, title_appendix=f'{k_folds}_folds_{data_fraction*100}%_of_data')\n",
    "    return cv_predictions\n",
    "\n",
    "def estimate_hyperparams(df, pipeline, scoring, label, k_folds=3, parameters={}, data_fraction: float = 1.0):\n",
    "    global model_counter\n",
    "    data = df.copy()\n",
    "    if(data_fraction and data_fraction < 1.0):\n",
    "        data = get_sample(df, data_fraction)\n",
    "\n",
    "    features, target = split_feature_target(data)\n",
    "    \n",
    "    # create the grid search instance\n",
    "    grid_search_estimator = GridSearchCV(pipeline, parameters, scoring=scoring, cv=k_folds, return_train_score=False, n_jobs=1)\n",
    "\n",
    "    # run the grid search\n",
    "    grid_search_estimator.fit(features, target)\n",
    "    \n",
    "    cv_results_df = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "    cv_results_df.to_excel(f'./results/model_{model_counter}_{label}_nestedCV_{data_fraction*100}%_of_data.xlsx')\n",
    "    model_counter = model_counter + 1\n",
    "    display(grid_search_estimator.best_params_)\n",
    "    display(cv_results_df)\n",
    "\n",
    "    return grid_search_estimator\n",
    "\n",
    "def generate_param_combinations(parameters):\n",
    "    '''list of list required for cross product of two lists'''\n",
    "    return list(ParameterGrid(parameters))\n",
    "\n",
    "def save_model(features, target, predictions, label, title_appendix: str = ''):\n",
    "    global model_counter\n",
    "\n",
    "    predictions_df = pd.DataFrame({'predicted yards_gained': predictions})\n",
    "    save_model = pd.concat([features, target, predictions_df], axis=1)\n",
    "\n",
    "    if (title_appendix != ''):\n",
    "        title_appendix = '_'+title_appendix\n",
    "    save_model.to_excel(f'./results/model_{model_counter}_{label}{title_appendix}.xlsx')\n",
    "\n",
    "    # increase counter for files\n",
    "    model_counter = model_counter + 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Regressor Class - with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithHistory(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mlp_params=None):\n",
    "        self.mlp_params = mlp_params\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.mlp_regressor = MLPRegressor(**(self.mlp_params or {}))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, X_val, y, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        for epoch in range(self.mlp_regressor.max_iter):\n",
    "            self.mlp_regressor.partial_fit(X, y)\n",
    "\n",
    "            # Calculate training loss\n",
    "            y_train_pred = self.mlp_regressor.predict(X)\n",
    "            training_loss = mean_squared_error(y, y_train_pred)\n",
    "            self.training_losses.append(training_loss)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            y_val_pred = self.mlp_regressor.predict(X_val)\n",
    "            validation_loss = mean_squared_error(y_val, y_val_pred)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.mlp_regressor.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"mlp_params\": self.mlp_params}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.mlp_params = params[\"mlp_params\"]\n",
    "        self.mlp_regressor.set_params(**self.mlp_params)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return -mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_df = PREPROCESSOR.run_df\n",
    "pass_df = PREPROCESSOR.pass_df\n",
    "\n",
    "run_features, run_target = split_feature_target(run_df)\n",
    "pass_features, pass_target = split_feature_target(pass_df)\n",
    "\n",
    "run_X_train, run_X_test, run_y_train, run_y_test = train_test_split(run_features, run_target)\n",
    "pass_X_train, pass_X_test, pass_y_train, pass_y_test = train_test_split(pass_features, pass_target)\n",
    "\n",
    "print(run_features.shape)\n",
    "print(pass_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(np.mean(run_df[TARGET_NAME]))\n",
    "display(np.mean(pass_df[TARGET_NAME]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_value = 40\n",
    "\n",
    "display(len(run_df[run_df[TARGET_NAME] > cutoff_value]))\n",
    "display(len(pass_df[pass_df[TARGET_NAME] > cutoff_value]))\n",
    "\n",
    "run_df = run_df[run_df[TARGET_NAME] < cutoff_value]\n",
    "pass_df = pass_df[pass_df[TARGET_NAME] < cutoff_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_y_test, run_predictions = test_model(pipeline, run_df, 1.0, label=LABEL_RUN)\n",
    "plot_coef(pipeline, LABEL_RUN)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_y_test, pass_predictions = test_model(pipeline, pass_df, 1.0, label=LABEL_PASS)\n",
    "plot_coef(pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for linear regression\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__fit_intercept': [True, False],\n",
    "    'regressor__random_state': [RANDOM_STATE],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, pipeline, 'neg_root_mean_squared_error', LABEL_RUN, parameters=parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pipeline, 'neg_root_mean_squared_error', LABEL_PASS, parameters=parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "run_params.update(logging_params)\n",
    "\n",
    "pass_params = pass_grid_search.best_params_\n",
    "pass_params.update(logging_params)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = run_pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, run_pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pass_pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pass_pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = run_pipeline.set_params(**run_params)\n",
    "run_predictions = test_model(run_pipeline, run_df, label = LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "plot_coef(run_pipeline, LABEL_RUN)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pass_pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model(pass_pipeline, pass_df, label = LABEL_PASS, data_fraction=1.0)\n",
    "\n",
    "plot_coef(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(Ridge())\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_y_test, run_predictions = test_model(pipeline, run_df, 1.0, label=LABEL_RUN)\n",
    "plot_coef(pipeline, LABEL_RUN)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_y_test, pass_predictions = test_model(pipeline, pass_df, 1.0, label=LABEL_PASS)\n",
    "plot_coef(pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for linear regression\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Ridge())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Ridge())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__alpha': [x / 10.0 for x in range(6, 15, 2)],\n",
    "    'regressor__fit_intercept': [True, False],\n",
    "    'regressor__max_iter': [1000, 7500, 15000],\n",
    "    'regressor__tol': [0.0001, 0.0005, 0.001],\n",
    "#    'regressor__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "    'regressor__random_state': [RANDOM_STATE],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, pipeline, 'neg_root_mean_squared_error', LABEL_RUN, parameters=parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pipeline, 'neg_root_mean_squared_error', LABEL_PASS, parameters=parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for linear regression\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Ridge())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Ridge())\n",
    "\n",
    "# same again for run and pass, as both have the same best params\n",
    "parameters = {\n",
    "    'regressor__alpha': [x / 10.0 for x in range(12, 21, 1)],\n",
    "    'regressor__fit_intercept': [True, False],\n",
    "    'regressor__max_iter': [800, 900, 1000, 1100, 1200],\n",
    "    'regressor__tol': [0.00008, 0.00009, 0.0001, 0.00011, 0.00012],\n",
    "    'regressor__random_state': [RANDOM_STATE],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, pipeline, 'neg_root_mean_squared_error', LABEL_RUN, parameters=parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pipeline, 'neg_root_mean_squared_error', LABEL_PASS, parameters=parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Ridge())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Ridge())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "run_params.update(logging_params)\n",
    "\n",
    "pass_params = pass_grid_search.best_params_\n",
    "pass_params.update(logging_params)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = run_pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, run_pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pass_pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pass_pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Ridge())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Ridge())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = run_pipeline.set_params(**run_params)\n",
    "run_predictions = test_model(run_pipeline, run_df, label = LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "plot_coef(run_pipeline, LABEL_RUN)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pass_pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model(pass_pipeline, pass_df, label = LABEL_PASS, data_fraction=1.0)\n",
    "\n",
    "plot_coef(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(Lasso())\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_y_test, run_predictions = test_model(pipeline, run_df, 1.0, label=LABEL_RUN)\n",
    "plot_coef(pipeline, LABEL_RUN)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_y_test, pass_predictions = test_model(pipeline, pass_df, 1.0, label=LABEL_PASS)\n",
    "plot_coef(pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for Lasso regression\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Lasso())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Lasso())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__alpha': [x / 10.0 for x in range(6, 15, 2)],\n",
    "    'regressor__fit_intercept': [True, False],\n",
    "    'regressor__max_iter': [800, 900, 1000, 1100, 1200],\n",
    "    'regressor__tol': [0.00005, 0.0001, 0.00015],\n",
    "    'regressor__warm_start': [True, False],\n",
    "    'regressor__random_state': [RANDOM_STATE],\n",
    "    'regressor__selection': ['cyclic', 'random'],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, pipeline, 'neg_root_mean_squared_error', LABEL_RUN, parameters=parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pipeline, 'neg_root_mean_squared_error', LABEL_PASS, parameters=parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Lasso())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Lasso())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "run_params.update(logging_params)\n",
    "\n",
    "pass_params = pass_grid_search.best_params_\n",
    "pass_params.update(logging_params)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = run_pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, run_pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pass_pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pass_pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Lasso())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(Lasso())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = run_pipeline.set_params(**run_params)\n",
    "run_predictions = test_model(run_pipeline, run_df, label = LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "plot_coef(run_pipeline, LABEL_RUN)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pass_pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model(pass_pipeline, pass_df, label = LABEL_PASS, data_fraction=1.0)\n",
    "\n",
    "plot_coef(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_y_test, run_predictions = test_model(pipeline, run_df, 1.0, label=LABEL_RUN)\n",
    "plot_coef(pipeline, LABEL_RUN)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_y_test, pass_predictions = test_model(pipeline, pass_df, 1.0, label=LABEL_PASS)\n",
    "plot_coef(pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for linear regression\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__fit_intercept': [True, False],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, pipeline, 'neg_root_mean_squared_error', LABEL_RUN, parameters=parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pipeline, 'neg_root_mean_squared_error', LABEL_PASS, parameters=parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "run_params.update(logging_params)\n",
    "\n",
    "pass_params = pass_grid_search.best_params_\n",
    "pass_params.update(logging_params)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = run_pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, run_pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pass_pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pass_pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(LinearRegression())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = run_pipeline.set_params(**run_params)\n",
    "run_predictions = test_model(run_pipeline, run_df, label = LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "plot_coef(run_pipeline, LABEL_RUN)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pass_pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model(pass_pipeline, pass_df, label = LABEL_PASS, data_fraction=1.0)\n",
    "\n",
    "plot_coef(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic model\n",
    "\n",
    "# make pipelines from preprocessing script\n",
    "poly_pipeline = Pipeline([('polynomialfeatures', PolynomialFeatures()), ('linear_regression', LinearRegression())])\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(poly_pipeline)\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(poly_pipeline)\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.25, label=LABEL_RUN)\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.25, label=LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new pipeliness from preprocessing script\n",
    "poly_pipeline = Pipeline([('polynomialfeatures', PolynomialFeatures()), ('linear_regression', LinearRegression())])\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(poly_pipeline)\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(poly_pipeline)\n",
    "\n",
    "parameters = {\n",
    "    'regressor__polynomialfeatures__degree': list(range(2,5)) + [(x, x) for x in range(2,5)],\n",
    "    'regressor__polynomialfeatures__interaction_only': [True, False],\n",
    "    'regressor__polynomialfeatures__include_bias': [True, False],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', LABEL_RUN, parameters=parameters, data_fraction=0.5)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', LABEL_PASS, parameters=parameters, data_fraction=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "logging_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "poly_pipeline = Pipeline([('polynomialfeatures', PolynomialFeatures()), ('linear_regression', LinearRegression())])\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(poly_pipeline)\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "run_params.update(logging_params)\n",
    "\n",
    "pass_params = pass_grid_search.best_params_\n",
    "pass_params.update(logging_params)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, run_pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pass_pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor(n_neighbors=3))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor(n_neighbors=3))\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.2)\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "parameters = {\n",
    "    'regressor__n_neighbors': range(5,10),\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'strict_columns': ['yardline_100', 'ydstogo', 'score_differential', 'td_prob', 'drive_play_count', 'drive_start_yard_line', 'spread_line', 'total_line', 'overall'],\n",
    "        },\n",
    "        {\n",
    "            'strict_columns': [],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(KNeighborsRegressor())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "run_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': False,\n",
    "    }\n",
    "}\n",
    "\n",
    "pass_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': False,\n",
    "    }\n",
    "}\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5))\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5))\n",
    "\n",
    "run_pipeline.set_params(**run_params)\n",
    "pass_pipeline.set_params(**pass_params)\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.25, label=LABEL_RUN)\n",
    "plot_feature_importances(run_pipeline, LABEL_RUN)\n",
    "\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, pass_df, 0.25, label=LABEL_PASS)\n",
    "plot_feature_importances(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__max_depth': [5, 10, 20],\n",
    "    'regressor__n_estimators': [20, 50],\n",
    "    'regressor__min_samples_split': [50, 100],\n",
    "    'regressor__min_samples_leaf': [10, 20],\n",
    "    'regressor__max_features': ['sqrt', 'log2', 1, None],\n",
    "    'regressor__random_state': [RANDOM_STATE],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', LABEL_RUN, k_folds=3, parameters=parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', LABEL_PASS, k_folds=3, parameters=parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further estimating\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "\n",
    "# representing the best params from last round / the area next to it\n",
    "run_parameters = {\n",
    "    'regressor__max_depth': range(8, 13),\n",
    "    'regressor__n_estimators': [45, 50, 55],\n",
    "    'regressor__min_samples_split': [40, 50, 60],\n",
    "    'regressor__min_samples_leaf': [20],\n",
    "    'regressor__max_features': [None],\n",
    "    'regressor__random_state': [RANDOM_STATE],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "pass_parameters = {\n",
    "    'regressor__max_depth': range(8, 13),\n",
    "    'regressor__n_estimators': [45, 50, 55],\n",
    "    'regressor__min_samples_split': [90, 100, 110],\n",
    "    'regressor__min_samples_leaf': [10],\n",
    "    'regressor__max_features': ['sqrt'],\n",
    "    'regressor__random_state': [RANDOM_STATE],\n",
    "    'outlier_remover__kw_args': [\n",
    "        {\n",
    "            'save_stats': False,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', LABEL_RUN, k_folds=3, parameters=run_parameters, data_fraction=1.0)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', LABEL_PASS, k_folds=3, parameters=pass_parameters, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation with best hyperparameters\n",
    "\n",
    "logging_params = {\n",
    "    'outlier_remover__kw_args': {\n",
    "        'save_stats': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(RandomForestRegressor())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "run_params.update(logging_params)\n",
    "\n",
    "pass_params = pass_grid_search.best_params_\n",
    "pass_params.update(logging_params)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "run_pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, run_pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pass_pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pass_pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(run_pipeline, LABEL_RUN)\n",
    "\n",
    "plot_feature_importances(run_pipeline, LABEL_RUN)\n",
    "\n",
    "# Plot the decision tree for passes\n",
    "plot_decision_tree(run_pipeline, LABEL_RUN)\n",
    "\n",
    "# Plot the decision tree for runs\n",
    "plot_decision_tree(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "# make pipelines\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    xgb.XGBRegressor(\n",
    "        learning_rate = 0.022,\n",
    "        n_estimators  = 1000,\n",
    "        max_depth     = 8,\n",
    "        eval_metric='rmsle'\n",
    "                           )\n",
    ")\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    xgb.XGBRegressor(\n",
    "        learning_rate = 0.015,\n",
    "        n_estimators  = 1000,\n",
    "        max_depth     = 8,\n",
    "        eval_metric='rmsle'\n",
    "        )\n",
    ")\n",
    "\n",
    "# test model and save predictions\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.05)\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, run_df, 0.05)\n",
    "\n",
    "plot_feature_importances(run_pipeline, LABEL_RUN)\n",
    "plot_feature_importances(pass_pipeline, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "\n",
    "parameters = {\n",
    "    \"regressor__max_depth\":    [8, 10],\n",
    "    \"regressor__n_estimators\": [1000, 1100],\n",
    "    \"regressor__learning_rate\": [0.022, 0.015]\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(xgb.XGBRegressor())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree for passes\n",
    "plot_decision_tree_xgb(run_pipeline, LABEL_PASS)\n",
    "\n",
    "# Plot the decision tree for runs\n",
    "plot_decision_tree_xgb(pass_pipeline, LABEL_RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    MLPWithHistory(\n",
    "        mlp_params={'hidden_layer_sizes': (50,),\n",
    "  'activation': 'relu',\n",
    "  'solver': 'adam',\n",
    "  'max_iter': 100}\n",
    "    )\n",
    ")\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(\n",
    "    MLPWithHistory(\n",
    "        mlp_params={'hidden_layer_sizes': (50,),\n",
    "  'activation': 'relu',\n",
    "  'solver': 'adam',\n",
    "  'max_iter': 100}\n",
    "    )\n",
    ")\n",
    "\n",
    "# estimate run model\n",
    "run_y_test, run_predictions = test_model(run_pipeline, run_df, 0.2)\n",
    "run_mlp = run_pipeline.named_steps['regressor']\n",
    "plot_train_val_loss(run_mlp.training_losses, run_mlp.validation_losses, LABEL_RUN)\n",
    "\n",
    "# estimate pass model\n",
    "pass_y_test, pass_predictions = test_model(pass_pipeline, pass_df, 0.2)\n",
    "pass_mlp = pass_pipeline.named_steps['regressor']\n",
    "plot_train_val_loss(pass_mlp.training_losses, pass_mlp.validation_losses, LABEL_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating hyperparameters\n",
    "\n",
    "# make new pipeliness from preprocessing script\n",
    "run_pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "pass_pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "\n",
    "parameters = {\n",
    "    'regressor__mlp_params': generate_param_combinations({\n",
    "        'hidden_layer_sizes': [(10,), (50,), (10,5), (20,10)], \n",
    "        'activation': ['relu'], \n",
    "        'solver': ['adam'], \n",
    "        'max_iter': [100] \n",
    "    })\n",
    "}\n",
    "\n",
    "run_grid_search = estimate_hyperparams(run_df, run_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)\n",
    "pass_grid_search = estimate_hyperparams(pass_df, pass_pipeline, 'neg_root_mean_squared_error', k_folds=3, parameters=parameters, data_fraction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PREPROCESSOR.make_preprocessing_pipeline(MLPWithHistory())\n",
    "\n",
    "run_params = run_grid_search.best_params_\n",
    "pass_params = pass_grid_search.best_params_\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**run_params)\n",
    "run_predictions = test_model_k_fold(run_df, pipeline, LABEL_RUN, data_fraction=1.0)\n",
    "\n",
    "# set params, test model and save predictions\n",
    "pipeline = pipeline.set_params(**pass_params)\n",
    "pass_predictions = test_model_k_fold(pass_df, pipeline, LABEL_PASS, data_fraction=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
